-- db.allium
--
-- The processing model of a single database within an XTDB node:
-- transaction submission → log processing → block flushing → query.
-- Compaction is specified separately: see compaction.allium.
--
-- Scope: one database (e.g. "xtdb" or a secondary).
-- External: the multi-database orchestration layer (attach/detach).

use "./trie-cat.allium" as trie_cat

------------------------------------------------------------
-- Context
------------------------------------------------------------

context {
    database: Database
    live_index: LiveIndex
    block_catalog: BlockCatalog
    table_catalog: TableCatalog
    log_processor: LogProcessor
}

------------------------------------------------------------
-- External Entities
------------------------------------------------------------

-- Truly external: managed by the multi-database layer or the environment.

external entity Client {
    -- The party submitting transactions.
}

external entity Log {
    -- An ordered, append-only message stream.
    -- Two instances per database: source-log and replica-log.
    -- Currently they MAY point to the same underlying log.
    -- See: api/log/Log.kt
}

external entity ObjectStore {
    -- Durable key-value storage for blocks, tries, table-blocks.
}

------------------------------------------------------------
-- Entities
------------------------------------------------------------

-- Behaviourally-relevant fields only; see linked sources for full definitions.

entity LiveIndex {
    -- See: indexer/LiveIndex.kt
    latest_completed_tx: TransactionKey?
    is_full: Boolean

    -- Derived
    has_data: latest_completed_tx != null
}

entity Database {
    -- See: indexer/Indexer.kt
    mode: read_write | read_only
    source_log: Log
    replica_log: Log
}

entity BlockCatalog {
    -- See: catalog/BlockCatalog.kt
    -- Single point of truth for the latest persisted block.
    latest_block: Block?

    -- Derived
    current_block_index: latest_block?.block_index
}

entity TableCatalog {
    -- See: catalog/TableCatalog.kt, table_catalog.clj
    -- Cumulative table metadata: row counts, column types, cardinality sketches.
    -- Accumulated across all blocks; used by the query planner.
    table_metadata: Map<trie_cat/TableRef, TableMetadata>
}

entity LogProcessor {
    -- See: indexer/LogProcessor.kt
    latest_processed_msg_id: MessageId
    last_flush_check: Instant
}

-- Storage.VERSION is a compile-time constant; storage_epoch is per-BufferPool.
-- The LogProcessor checks incoming TriesAdded messages against these values
-- to discard stale notifications from a previous epoch.
external value current_storage_version: Int   -- See: storage/Storage.kt (VERSION)
external value current_storage_epoch: Int     -- See: buffer_pool/BufferPool.kt (epoch)

------------------------------------------------------------
-- Values
------------------------------------------------------------

value MessageId {
    -- Epoch + offset packed into a long.
    -- See: util/MsgIdUtil.kt
    epoch: Int
    offset: Int
}

value TransactionKey {
    -- See: block/proto/block.proto (TxKey)
    tx_id: Long
    system_time: Instant
}

value Block {
    -- A persisted block in the object store.
    -- See: catalog/BlockCatalog.kt (BlockCatalog.buildBlock)
    block_index: Long
    latest_completed_tx: TransactionKey
    latest_processed_msg_id: MessageId
}

value Snapshot {
    -- A point-in-time read view of the database.
    as_of: TransactionKey
}

value TableMetadata {
    -- Per-table cumulative metadata, merged across blocks.
    -- See: table_catalog.clj
    row_count: Long?
}

------------------------------------------------------------
-- Log Messages
------------------------------------------------------------

-- The LogProcessor dispatches on message kind.
-- See: log/proto/log.proto (LogMessage oneof)

entity Message {
    msg_id: MessageId
    log_timestamp: Instant
    kind: TxMessage | FlushBlockMessage | TriesAddedMessage | BlockUploadedMessage | AttachDbMessage | DetachDbMessage
}

variant TxMessage : Message {
    -- Client-submitted transaction operations.
    -- See: tx/TxWriter.kt, api/log/Log.kt (Log$Message$Tx) — serialised via TxWriter, not proto.
    system_time: Instant?
    default_tz: String?
    user: String?
}

variant FlushBlockMessage : Message {
    -- Request to finish the current block.
    -- See: log/proto/log.proto (FlushBlock)
    expected_block_idx: Long?
}

variant TriesAddedMessage : Message {
    -- Notification that new tries exist (from block flush or compaction).
    -- See: log/proto/log.proto (TriesAdded)
    storage_version: Int
    storage_epoch: Int
    tries: List<trie_cat/TrieDetails>
}

variant BlockUploadedMessage : Message {
    -- Notification that a block has been written to the object store.
    -- Sent by the writer after finishBlock(); consumed by read-only nodes to refresh catalogs without polling.
    -- See: log/proto/log.proto (BlockUploaded)
    block_index: Long
    latest_processed_msg_id: MessageId
    storage_epoch: StorageEpoch
}

variant AttachDbMessage : Message {
    -- See: log/proto/log.proto (AttachDatabase)
    db_name: String
}

variant DetachDbMessage : Message {
    -- See: log/proto/log.proto (DetachDatabase)
    db_name: String
}

------------------------------------------------------------
-- Config
------------------------------------------------------------

config {
    flush_timeout: Duration = 4.hours
    max_block_rows: Integer = 102400
}

------------------------------------------------------------
-- Rules
------------------------------------------------------------

-- == Transaction Submission ==

rule ClientSubmitsTransaction {
    when: ClientSubmitsTx(client, database, tx_ops, opts)

    requires: database.mode == read_write

    ensures:
        let msg = TxMessage.created(
            system_time: opts.system_time,
            default_tz: opts.default_tz,
            user: opts.user
        )
        msg appended to database.source_log
}


-- == Log Processing ==
--
-- LogProcessor subscribes to the source-log and processes messages sequentially.
-- This is the heart of the system.

rule ProcessTxMessage {
    -- A transaction message arrives from the source log.
    when: msg: TxMessage appended to database.source_log

    requires: msg.msg_id > log_processor.latest_processed_msg_id

    ensures:
        -- The indexer resolves and applies the transaction.
        -- Result is either committed or aborted.
        let result = IndexTransaction(msg)
        log_processor.latest_processed_msg_id = msg.msg_id

        -- If the live index is now full, a block flush is needed.
        if live_index.is_full:
            BlockFlushNeeded(msg.log_timestamp)
}

rule ProcessFlushBlockMessage {
    when: msg: FlushBlockMessage appended to database.source_log

    requires: msg.msg_id > log_processor.latest_processed_msg_id
    requires: msg.expected_block_idx == block_catalog.current_block_index

    ensures:
        log_processor.latest_processed_msg_id = msg.msg_id
        BlockFlushNeeded(msg.log_timestamp)
}

rule ProcessTriesAddedMessage {
    -- Tries-added messages are idempotent notifications.
    -- They arrive on the replica-log but currently also on the source-log (because both point to the same underlying log).
    when: msg: TriesAddedMessage appended to database.source_log

    requires: msg.msg_id > log_processor.latest_processed_msg_id
    requires: msg.storage_version == current_storage_version
    requires: msg.storage_epoch == current_storage_epoch

    ensures:
        trie_cat/trie_catalog.addTries(msg.tries, msg.log_timestamp)
        log_processor.latest_processed_msg_id = msg.msg_id
}


-- == Block Flushing ==
--
-- Two paths: the leader writes blocks and notifies via BlockUploaded on the source log;
-- read-only nodes buffer messages until BlockUploaded arrives, then transition and replay.

rule LeaderFinishesBlock {
    -- log_timestamp is the log timestamp of the message that triggered the flush
    -- (either the transaction that filled the block, or a FlushBlock message).
    -- This becomes the as_of for trie registration, used later by GC to determine
    -- deletion eligibility. The 24-hour garbage_lifetime grace period absorbs
    -- any practical difference between these two sources.
    when: BlockFlushNeeded(log_timestamp)

    requires: database.mode == read_write

    let block_index = (block_catalog.current_block_index ?? -1) + 1
    -- finishBlock returns a per-table map of finished blocks,
    -- each containing tries and table metadata.
    -- See: indexer/LiveIndex.kt (finishBlock), indexer/LiveTable.kt (FinishedBlock)
    let finished_tables = live_index.finishBlock(block_index)
    let tries = finished_tables.collectTries()

    ensures:
        -- Write tries-added to the replica-log so followers can see them.
        let tries_added = TriesAddedMessage.created(tries: tries)
        tries_added appended to database.replica_log

        -- Register tries locally.
        trie_cat/trie_catalog.addTries(tries, log_timestamp)

        -- Update table catalog with new metadata from finished tables.
        -- The leader updates in-memory via finishBlock (merges delta metadata);
        -- the follower achieves the same result via refresh (reloads from object store).
        -- See: table_catalog.clj (finishBlock), TableCatalog (refresh)
        table_catalog.finishBlock(finished_tables)

        -- Persist block to object store.
        Block.created(
            block_index: block_index,
            latest_completed_tx: live_index.latest_completed_tx,
            latest_processed_msg_id: log_processor.latest_processed_msg_id
        )

        block_catalog.refresh(block)

        -- Notify read-only nodes that the block is available.
        let block_uploaded = BlockUploadedMessage.created(
            block_index: block_index,
            latest_processed_msg_id: log_processor.latest_processed_msg_id,
            storage_epoch: buffer_pool.epoch
        )
        block_uploaded appended to database.source_log

        -- Reset for next block.
        live_index.nextBlock()

        -- Wake up compaction.
        CompactorSignalled()
}

rule FollowerBuffersUntilBlockEnd {
    -- The follower cannot process further txs while the live index is full.
    -- It consumes messages from the source log, buffering them until BlockUploaded arrives.
    when: BlockFlushNeeded(log_timestamp)

    requires: database.mode == read_only

    let block_index = (block_catalog.current_block_index ?? -1) + 1

    ensures:
        let (buffered, msg) = consumeUntil BlockUploadedMessage from database.source_log
            where msg.block_index == block_index
              and msg.storage_epoch == buffer_pool.epoch

        FollowerProcessesBlockEnd(block_index, buffered)
}

rule FollowerProcessesBlockEnd {
    when: FollowerProcessesBlockEnd(block_index, buffered)

    ensures:
        -- The reader doesn't write tries — it refreshes catalogs from what the writer wrote.
        block_catalog.refresh(block_index)
        table_catalog.refresh(block_index)
        trie_cat/trie_catalog.refresh(block_index)
        live_index.nextBlock()

        -- Replay buffered messages into the new live index.
        for each buffered_msg in buffered:
            ProcessMessage(buffered_msg)
}


-- == Flush Timeout ==
--
-- If transactions have been accumulating without a block flush,
-- the leader sends itself a FlushBlock message.

rule FlushTimeoutFires {
    when: log_processor: LogProcessor.last_flush_check + config.flush_timeout <= now

    requires: database.mode == read_write
    requires: live_index.has_data
    requires: block_catalog.current_block_index unchanged since log_processor.last_flush_check

    ensures:
        FlushBlockMessage.created(
            expected_block_idx: block_catalog.current_block_index
        ) appended to database.source_log
}


-- == Query Serving ==

rule ClientOpensSnapshot {
    when: ClientOpensSnapshot(client, database)

    ensures:
        -- A snapshot sees all committed transactions up to this point.
        -- The snapshot is a consistent read of live index + block data.
        Snapshot.created(
            as_of: live_index.latest_completed_tx
        )
}


-- == Error Handling ==

rule ProcessingErrorHaltsNode {
    -- Any unrecoverable error during log processing is fatal.
    -- The node is marked unhealthy; orchestration should restart it.
    when: ProcessingError(msg_id, error)

    ensures:
        -- All pending watchers are notified of the error.
        -- LogProcessor stops permanently.
        -- No partial state corruption: either a message is fully processed or not at all.
        NodeMarkedUnhealthy()
}

------------------------------------------------------------
-- Surfaces
------------------------------------------------------------

actor Client {
    identified_by: external session
}

surface TransactionSubmission {
    for caller: Client

    context db: Database

    provides:
        ClientSubmitsTx(caller, db, tx_ops, opts)
            when db.mode == read_write

    invariant: OrderedProcessing
        -- Transactions are processed in the order they appear in the source log, not the order they were submitted.
}

surface QueryAccess {
    for caller: Client

    context db: Database

    provides:
        ClientOpensSnapshot(caller, db)

    invariant: SnapshotIsolation
        -- A snapshot sees a consistent point-in-time view.
        -- It includes all transactions up to latest_completed_tx.
}

------------------------------------------------------------
-- Deferred Specifications
------------------------------------------------------------

deferred IndexTransaction           -- transaction resolution (put/delete/erase logic)

------------------------------------------------------------
-- Open Questions
------------------------------------------------------------

open_question "Should the flush timeout be per-database or global?"
open_question "What happens if a read-only node's latestProcessedMsgId is behind the block? (currently a Fault)"
