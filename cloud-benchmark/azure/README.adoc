= Azure Auctionmark Benchmark

Contained within this folder are a number of things for setting up & running a cluster of containerized auctionmark running worker tasks on Azure Platform.

Within this README is the following:

* Instructions for setting up the docker image.
* How to setup the infra necessary on Azure.
* How to push the docker image to Azure.
* How to deploy the benchmark containers.

== Requirements

For local development, you will need the following:

* To run the various scripts and commands within this folder, you will need the `az` command line tool, and to be authenticated with it.
* `docker` available on your machine.
* `docker buildx` configured.
* `terraform` available on your machine.
* `kubectl` available on your machine.

=== Authenticating with Azure

Firstly, you want to login to Azure using the command line - this can be done by running:
```bash
az login --scope https://management.azure.com//.default
```

After this, you want to ensure that you are using the correct subscription - in our case, that will be "XTDB long-run reliability":
```bash
az account set --subscription "XTDB long-run reliability"
```

== Setting up the Azure Infra

All of the config for setting up the infrastructure on Azure is handled by Terraform - and the files are within the `terraform` directory. 

* The `main.tf` file contains the main configuration for the Azure resources.
* Can update the names of a few things within `terraform.tfvars` - though this is not expected to change so much from the default values. The only thing that may be expected to change is the `kubernetes_vm_size` if you wish to run more than one pod/multiple nodes.

Prior to deploying any of the infra using terraform, you will need to initialize the terraform directory. From the `terraform` directory:
```bash
terraform init
```

To ensure that the terraform config is properly formatted, you can run:
```bash
terraform fmt
```

To then validate the terraform config, you can run:
```bash
terraform validate
```

To see what will will/change when running terraform, you can run:
```bash
terraform plan
```

To actually deploy the infra, you can run:
```bash
terraform apply
```

This will setup all of the azure resources we use and a base 

== Building & Pushing the Docker Image to Azure

To push the docker image to Azure, you will need to use the Azure Container Registry.

To login to the Azure Container Registry, you can run:
```bash
az acr login --name cloudbenchmarkregistry
```

You can then build & push the docker image for azure by running the following script:

```bash
./scripts/build-azure-bench.sh
```

This will:

* Creates a Shadow JAR of the `cloud-benchmark:azure` project.
** This contains the compiled `cloud-benchmark` project, which has a main file that runs auctionmark & configures it with provided node config, and also any of the dependencies we require to run the Azure module.
* Creates a docker image, copying in the Shadow JAR and the `azure-config.yaml` file with the node config.
** If you want to set any JVM opts - do it by editing the Dockerfile's `ENTRYPOINT`.
* Pushes the docker image to the Azure Container Registry.


== Deploying the Benchmark Containers


Before trying to deploy the benchmark containers, you will need to ensure that you have the necessary permissions to deploy to the Kubernetes cluster. You can do this by running the following command:
```
az aks get-credentials --resource-group cloud-benchmark-resources --name cloud-benchmark-cluster
```

This will configure your `kubectl` to use the credentials for the Kubernetes cluster. 

Before deploying anything, must ensure all of the necessary pieces of config are set within the ConfigMap/match the values/names of the terraform architecture, run the following in the terraform directory:
```
terraform outputs
```

Ensure that the contents of the files under `kubernetes` match the values of the terraform outputs.

We must also ensure that we have both a namespace and a service account for the benchmark to run under. To create these, you can run the following:
```
kubectl create namespace cloud-benchmark
kubectl create serviceaccount xtdb-service-account --namespace cloud-benchmark 
```

The config for the deployments/jobs live under `kubernetes` - within these, you can set/configure any of the necessary parameters for running the image on the cluster. When ready to run a single node auctionmark job, run the following command from the base of this directory:
```
kubectl apply -f kubernetes/single-node-auctionmark.yaml
```

You can see the status of job creation using the following:
```
kubectl get jobs --namespace cloud-benchmark
```

Trail the logs of the single node run by running:
```
kubectl logs job.batch/xtdb-single-node-auctionmark --namespace cloud-benchmark -f
```

=== Multinode Benchmarking

Running a multi-node benchmark is similar, though requires a few more steps.

Firstly, update the terraform config to have the correct amount of memory/cpu for the nodes you wish to run. 

* This can be altered with the `kubernetes_vm_size` variable in the `terraform.tfvars` file.
* You will need to set a `temporary_name_for_rotation` when resizing the node pool.

We will also need to update the node config and push a new image, as we are now using Kafka as the txLog and running multiple nodes - update the contents of the `azure-config.yaml` file:

* Switch the txLog implementation to `kafka`, adding the relevant config/env vars.
* Set a reasonable maxDiskCachePercentage size depending on number of nodes running.

After this, push a new image to the Azure Container Registry. 

To set up all the necessary kafka pods that will be used/shared by the nodes txLog, run the following:

```
kubectl apply -f kubernetes/kafka.yaml
```

This will setup a kafka service the pods can connect to, alongside the persistent volume claim for kafka.

When the kafka pods are ready, you can then deploy the multi-node benchmark by running the following:
```
kubectl apply -f kubernetes/multi-node-auctionmark.yaml
```


== Clear up between runs

If you want to totally clear up data between runs, you'll want to do the following:

* Clear up the job/pods
** Clear up the Kafka deployment (if running a multi-node benchmark)
* Empty the Azure Storage Blobs Container
* Delete the Persistent Storage volume used by the TxLog
** Delete the Persistent Storage volume used by Kafka (if running a multi-node benchmark)
* Delete the Persistent Storage volume containing the Local Disk Caches

.Clear up the XTDB job

To clear up the single node workload, you can run:
```bash
kubectl delete jobs xtdb-single-node-auctionmark --namespace cloud-benchmark
```

For the multi-node workload, you can run:
```bash
kubectl delete jobs xtdb-multi-node-auctionmark --namespace cloud-benchmark
```

.Clear up the Kafka deployment

To clear up the Kafka deployment, you can run:
```bash
kubectl delete deployment kafka-app --namespace cloud-benchmark
```

.Command to empty the Azure Storage Blob Container:
```bash
./scripts/clear-azure-storage.sh
```

.Deleting Persistent Storage Volumes:
You can remove the Persistent Storage volumes within the google cloud UI, but will need to be careful to ensure they are both removed from GKE and deleted within Compute Engine's storage as well. You will need to ensure any pods are closed/deleted first, and then to delete them, you can do the following:
```bash
kubectl delete pvc xtdb-pvc-log --namespace cloud-benchmark
kubectl delete pvc xtdb-pvc-local-caches --namespace cloud-benchmark
# If running a multi-node benchmark with Kafka
kubectl delete pvc kafka-pvc --namespace cloud-benchmark
``` 

There is a helper script to do _all_ of the above, which can be run by:
```bash
./scripts/clear-azure-bench.sh
```

== Monitoring With Azure Monitor

=== Setup

To monitor the benchmark, we can use Azure Application Insights. We set up an applications insights resource in the terraform configuration, and output a connection string.

=== Collecting metrics from node

To see the benchmark metrics you need to supply an application insights connection string via an environment variable in the `single-node-auctionmark.yaml` or `multi-node-auctionmark.yaml` file as below:

```yaml
apiVersion: "v1"
kind: "ConfigMap"
metadata:
  ...
data:
  XTDB_AZURE_APP_INSIGHTS_CONNECTION_STRING: "<connection-string>"
  ...
```

You can retrieve the connection_string from terraform state as follows:

```bash
terraform output -raw insights_connection_string
```

After a while you should be able to see the metrics in the Azure portal under the Application Insights resource, navigating to Monitoring > Metrics and looking under Metric Namespace > Custom.

=== Observing metrics in the Dashboard

We have made a custom dashboard for various XTDB, JVM and auctionmark metrics that we care about, with the ability to split them by node.

To setup the dashboard itself, there's a few steps we must take:

* Before anything else - ensure we can filter/split custom metrics. This can be found under the `usage and estimated costs` section of the Application Insights resource, and we should update this to allow "Alerting on Custom Metric Dimensions".
* With that setup, we can create our custom dashboard.
** Got to `Application Dashbard` on the Application Insights resource.
** Create a new custom dashboard.
** We can upload this from a template - you can use the template found within the `cloud-benchmark/azure` directory, `application-insights/dashboard.json`. 

We already have a shared dashboard setup, under "Monitoring Dashboard"

== Monitoring with Grafana

Within here are also some provided templates for setting up a Grafana-Otel deployment which shall scrape the pods from the XTDB benchmark Job.

To deploy grafana, simply run:
```
kubectl apply -f kubernetes/grafana.yaml
```

To access the Grafana instance, you can use the external IP of the LoadBalancer service created for Grafana:
```bash
kubectl get svc grafana-service --namespace cloud-benchmark
```

The Grafana dashboard can be accessed via the external IP of the LoadBalancer service, on port `3001`. The default credentials are `admin`/`admin`.

With this up and runing, you can then import the XTDB dashboards (the cluster monitoring dashboard and node debugging dashboard) from `monitoring/grafana/dashboards`, and use these to monitor the benchmark pods.

=== Clearing up Grafana

If using the `clear-azure-bench.sh` script, by default we retain the Grafana deployment. If you wish to clear this up, you can run:

```bash
./scripts/clear-azure-bench --clear-grafana
```

This will clear up the Grafana deployment, the Grafana persistent volume claim, and the Prometheus persistent volume claim.

== Trailing the Logs

You can trail the logs of each node within the job by running:

```
kubectl logs job.batch/xtdb-multi-node-auctionmark --namespace cloud-benchmark -c <container-name> -f
```

For example, to see the logs from the load-phase, run:
```
kubectl logs job.batch/xtdb-multi-node-auctionmark --namespace cloud-benchmark -c load-phase -f
```

To see the logs of one of the nodes, run:
```
kubectl logs job.batch/xtdb-multi-node-auctionmark --namespace cloud-benchmark -c xtdb-bench-1 -f
```
