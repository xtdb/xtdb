name: Run Nightly Benchmark

on:
  workflow_call:
    inputs:
      bench_type:
        description: 'Benchmark type passed to Helm'
        required: true
        type: string
      scale_factor:
        description: 'Scale factor applied to the benchmark chart values'
        required: false
        default: ''
        type: string
      job_display_name:
        description: 'Label to show in the job name and Slack notifications'
        required: true
        type: string
      timeout_minutes:
        description: 'Job timeout in minutes (GitHub Actions workflow timeout)'
        required: false
        default: 90
        type: number
      benchmark_timeout:
        description: 'Benchmark timeout as ISO-8601 duration (e.g. PT1H30M). If empty, no timeout is applied.'
        required: false
        default: ''
        type: string
      branch:
        description: 'Branch to build the benchmark image from'
        required: false
        default: 'main'
        type: string
      image_owner:
        description: 'Docker registry owner/organization for the benchmark image (defaults to repository owner)'
        required: false
        default: ''
        type: string
      duration:
        description: 'Duration for benchmarks that support it (e.g. PT30M for AuctionMark)'
        required: false
        default: ''
        type: string
      threads:
        description: 'Thread count for benchmarks that support it'
        required: false
        default: ''
        type: string
      node_count:
        description: 'Node count for benchmarks that support it'
        required: false
        default: ''
        type: string
      readings_device_count:
        description: 'Device count for readings benchmark'
        required: false
        default: ''
        type: string
      reading_count:
        description: 'Reading count for readings benchmark'
        required: false
        default: ''
        type: string
      clickbench_size:
        description: 'Dataset size for clickbench (tiny, small, medium, full)'
        required: false
        default: ''
        type: string
      clickbench_limit:
        description: 'Row limit for clickbench'
        required: false
        default: ''
        type: string
      tsbs_device_count:
        description: 'Device count for TSBS IoT benchmark'
        required: false
        default: ''
        type: string
      tsbs_query_iterations:
        description: 'Query iterations per type for TSBS IoT benchmark'
        required: false
        default: ''
        type: string
      tsbs_workers:
        description: 'Parallel query workers for TSBS IoT benchmark'
        required: false
        default: ''
        type: string
      tsbs_burn_in:
        description: 'Burn-in queries (discarded from stats) for TSBS IoT benchmark'
        required: false
        default: ''
        type: string
      ingest_tx_overhead_doc_count:
        description: 'Document count for ingest-tx-overhead benchmark'
        required: false
        default: ''
        type: string
      ingest_tx_overhead_batch_sizes:
        description: 'Batch sizes for ingest-tx-overhead benchmark (comma-separated)'
        required: false
        default: ''
        type: string
      patch_doc_count:
        description: 'Document count for patch benchmark'
        required: false
        default: ''
        type: string
      patch_patch_count:
        description: 'Patch count for patch benchmark'
        required: false
        default: ''
        type: string
      products_limit:
        description: 'Limit for products benchmark (empty = all)'
        required: false
        default: ''
        type: string
      ts_devices_size:
        description: 'Dataset size for ts-devices benchmark (small, med, big)'
        required: false
        default: ''
        type: string
      fusion_device_count:
        description: 'Device count for fusion benchmark'
        required: false
        default: ''
        type: string
      fusion_reading_count:
        description: 'Reading count for fusion benchmark'
        required: false
        default: ''
        type: string
      fusion_duration:
        description: 'OLTP phase duration for fusion benchmark'
        required: false
        default: ''
        type: string
      fusion_threads:
        description: 'OLTP thread count for fusion benchmark'
        required: false
        default: ''
        type: string
      run_queue:
        description: 'Whether this run is part of the nightly benchmark queue'
        required: false
        default: 'false'
        type: string

permissions:
  id-token: write
  contents: read
  actions: write
  packages: write

jobs:
  build-benchmark-image:
    uses: ./.github/workflows/bench-docker.yml
    with:
      branch: ${{ inputs.branch }}
      image_owner: ${{ inputs.image_owner }}

  benchmark:
    needs: build-benchmark-image
    name: "${{ inputs.job_display_name }} (SF: ${{ inputs.scale_factor }})"
    runs-on: ubuntu-latest
    timeout-minutes: ${{ inputs.timeout_minutes }}
    env:
      BENCH_TYPE: ${{ inputs.bench_type }}
      SCALE_FACTOR: ${{ inputs.scale_factor }}
      SLACK_LABEL: ${{ inputs.job_display_name }}
      GRAFANA_ADMIN_PASSWORD: ${{ secrets.BENCHMARK_GRAFANA_ADMIN_PASSWORD }}

    steps:
      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v5

      - name: Cache Babashka
        id: cache-bb
        uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
        with:
          path: /usr/local/bin/bb
          key: babashka-1.12.209

      - name: Install Babashka
        if: steps.cache-bb.outputs.cache-hit != 'true'
        run: |
          set -euo pipefail
          BABASHKA_VERSION=1.12.209
          curl -sSfL "https://github.com/babashka/babashka/releases/download/v${BABASHKA_VERSION}/babashka-${BABASHKA_VERSION}-linux-amd64.tar.gz" -o /tmp/babashka.tar.gz
          sudo tar -xzf /tmp/babashka.tar.gz -C /usr/local/bin bb
          rm -f /tmp/babashka.tar.gz
          bb --version

      - name: Azure CLI Login
        uses: azure/login@a457da9ea143d694b1b9c7c869ebb04ebe844ef5 # v2
        with:
          client-id: ${{ secrets.BENCHMARK_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.BENCHMARK_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.BENCHMARK_AZURE_SUBSCRIPTION_ID }}

      - name: Acquire Kubernetes Configuration
        run: |
          az aks get-credentials --resource-group cloud-benchmark-resources --name xtdb-bench-cluster

      - name: Check Existing Deployment
        id: check-deployment
        continue-on-error: true
        run: |
          # exit 0 if exists, non-zero if not
          helm status xtdb-benchmark -n cloud-benchmark >/dev/null 2>&1

      - name: Inspect Existing Deployment
        if: steps.check-deployment.outcome == 'success'
        id: inspect-deployment
        run: |
          set -euo pipefail
          result=$(./modules/bench/cloud/scripts/tasks inspect-deployment)
          echo "deployment_status=$(echo "$result" | jq -r '.status')" >> "$GITHUB_OUTPUT"
          echo "job_names=$(echo "$result" | jq -c '.jobNames')" >> "$GITHUB_OUTPUT"
          echo "terminal_job_names=$(echo "$result" | jq -c '.terminalJobs')" >> "$GITHUB_OUTPUT"
          echo "failed_job_names=$(echo "$result" | jq -c '.failedJobs')" >> "$GITHUB_OUTPUT"
          echo "succeeded_job_names=$(echo "$result" | jq -c '.succeededJobs')" >> "$GITHUB_OUTPUT"

      - name: Re-authenticate Azure CLI (before cleanup)
        if: steps.inspect-deployment.outputs.deployment_status == 'completed' || steps.inspect-deployment.outputs.deployment_status == 'failed'
        uses: azure/login@a457da9ea143d694b1b9c7c869ebb04ebe844ef5 # v2
        with:
          client-id: ${{ secrets.BENCHMARK_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.BENCHMARK_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.BENCHMARK_AZURE_SUBSCRIPTION_ID }}

      - name: Cleanup Existing Deployment
        if: steps.inspect-deployment.outputs.deployment_status == 'completed' || steps.inspect-deployment.outputs.deployment_status == 'failed'
        run: |
          set -euo pipefail
          echo "Previous benchmark finished with status: ${{ steps.inspect-deployment.outputs.deployment_status }}. Cleaning up release before proceeding."
          ./modules/bench/cloud/clear-bench.sh azure

      - name: Cancel workflow (existing deployment)
        if: steps.check-deployment.outcome == 'success' && steps.inspect-deployment.outputs.deployment_status == 'in_progress'
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            core.notice('Existing deployment found and still in progress. Cancelling this workflow run to avoid overlap.');
            await github.rest.actions.cancelWorkflowRun({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId,
            });

      - name: Stop job after cancellation
        if: steps.check-deployment.outcome == 'success' && steps.inspect-deployment.outputs.deployment_status == 'in_progress'
        run: |
          echo "Workflow cancellation requested due to existing deployment. Stopping job."
          exit 0

      - name: Re-authenticate Azure CLI (before clear state)
        if: steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress'
        uses: azure/login@a457da9ea143d694b1b9c7c869ebb04ebe844ef5 # v2
        with:
          client-id: ${{ secrets.BENCHMARK_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.BENCHMARK_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.BENCHMARK_AZURE_SUBSCRIPTION_ID }}

      - name: Clear Bench State
        if: steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress'
        continue-on-error: true
        run: |
          echo "Ensuring clean state before starting benchmark..."
          ./modules/bench/cloud/clear-bench.sh azure || true

      - name: Install Monitoring Stack
        if: steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress'
        continue-on-error: true
        run: |
          bash ./modules/bench/cloud/monitoring/install-monitoring.sh

      - name: Create Bench Secret (GITHUB_PAT)
        if: steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress'
        run: |
          set -euo pipefail
          kubectl -n cloud-benchmark create secret generic xtdb-bench-secrets \
            --from-literal=GITHUB_PAT="${{ secrets.BENCHMARK_GITHUB_PAT }}" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Create AWS Datasets Secret (for S3 dataset benchmarks)
        if: (steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress') && (inputs.bench_type == 'clickbench' || inputs.bench_type == 'products' || inputs.bench_type == 'ts-devices')
        run: |
          set -euo pipefail
          kubectl -n cloud-benchmark create secret generic xtdb-aws-datasets-secret \
            --from-literal=AWS_ACCESS_KEY_ID="${{ secrets.BENCHMARK_DATASETS_AWS_ACCESS_KEY_ID }}" \
            --from-literal=AWS_SECRET_ACCESS_KEY="${{ secrets.BENCHMARK_DATASETS_AWS_SECRET_ACCESS_KEY }}" \
            --from-literal=AWS_REGION="${{ secrets.BENCHMARK_DATASETS_AWS_REGION || 'eu-west-1' }}" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Record Deployment Start
        if: steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress'
        run: echo "DEPLOY_START=$(date -u +%s)" >> "$GITHUB_ENV"

      - name: Run Benchmark
        if: steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress'
        env:
          BENCH_TYPE: ${{ env.BENCH_TYPE }}
          SCALE_FACTOR: ${{ env.SCALE_FACTOR }}
          IMAGE_TAG: ${{ needs.build-benchmark-image.outputs.image_tag }}
          IMAGE_OWNER: ${{ inputs.image_owner || github.repository_owner }}
          GIT_SHA: ${{ github.sha }}
          GIT_BRANCH: ${{ inputs.branch }}
          GIT_REPO: ${{ github.repository }}
          GITHUB_RUN_ID: ${{ github.run_id }}
          DURATION: ${{ inputs.duration }}
          THREADS: ${{ inputs.threads }}
          NODE_COUNT: ${{ inputs.node_count }}
          READINGS_DEVICE_COUNT: ${{ inputs.readings_device_count }}
          READING_COUNT: ${{ inputs.reading_count }}
          BENCHMARK_TIMEOUT: ${{ inputs.benchmark_timeout }}
          CLICKBENCH_SIZE: ${{ inputs.clickbench_size }}
          CLICKBENCH_LIMIT: ${{ inputs.clickbench_limit }}
          TSBS_DEVICE_COUNT: ${{ inputs.tsbs_device_count }}
          TSBS_QUERY_ITERATIONS: ${{ inputs.tsbs_query_iterations }}
          TSBS_WORKERS: ${{ inputs.tsbs_workers }}
          TSBS_BURN_IN: ${{ inputs.tsbs_burn_in }}
          INGEST_TX_OVERHEAD_DOC_COUNT: ${{ inputs.ingest_tx_overhead_doc_count }}
          INGEST_TX_OVERHEAD_BATCH_SIZES: ${{ inputs.ingest_tx_overhead_batch_sizes }}
          PATCH_DOC_COUNT: ${{ inputs.patch_doc_count }}
          PATCH_PATCH_COUNT: ${{ inputs.patch_patch_count }}
          PRODUCTS_LIMIT: ${{ inputs.products_limit }}
          TS_DEVICES_SIZE: ${{ inputs.ts_devices_size }}
          FUSION_DEVICE_COUNT: ${{ inputs.fusion_device_count }}
          FUSION_READING_COUNT: ${{ inputs.fusion_reading_count }}
          FUSION_DURATION: ${{ inputs.fusion_duration }}
          FUSION_THREADS: ${{ inputs.fusion_threads }}
          RUN_QUEUE: ${{ inputs.run_queue }}
        run: |
          helm dependency update ./modules/bench/cloud/helm
          HELM_ARGS=(
            --namespace "cloud-benchmark"
            --create-namespace
            -f ./modules/bench/cloud/azure/values.yaml
            --set "benchType=${BENCH_TYPE}"
            --set "image.repository=ghcr.io/${IMAGE_OWNER}/xtdb-bench"
            --set "image.tag=${IMAGE_TAG}"
            --set "git.sha=${GIT_SHA}"
            --set "git.branch=${GIT_BRANCH}"
            --set "git.repo=${GIT_REPO}"
            --set "git.runId=${GITHUB_RUN_ID}"
            --set "providerConfig.existingSecret=xtdb-bench-secrets"
            --set "providerConfig.env.AZURE_USER_MANAGED_IDENTITY_CLIENT_ID=${{ secrets.BENCHMARK_AZURE_USER_MANAGED_IDENTITY_CLIENT_ID }}"
            --set "providerConfig.serviceAccountAnnotations.azure\.workload\.identity/client-id=${{ secrets.BENCHMARK_AZURE_USER_MANAGED_IDENTITY_CLIENT_ID }}"
          )

          # Add optional parameters if provided
          if [ -n "$SCALE_FACTOR" ]; then
            HELM_ARGS+=(--set "${BENCH_TYPE}.scaleFactor=${SCALE_FACTOR}")
          fi
          if [ -n "$DURATION" ]; then
            HELM_ARGS+=(--set "${BENCH_TYPE}.duration=${DURATION}")
          fi
          if [ -n "$THREADS" ]; then
            HELM_ARGS+=(--set "${BENCH_TYPE}.threads=${THREADS}")
          fi
          if [ -n "$NODE_COUNT" ]; then
            HELM_ARGS+=(--set "${BENCH_TYPE}.nodeCount=${NODE_COUNT}")
          fi
          if [ -n "$READINGS_DEVICE_COUNT" ]; then
            HELM_ARGS+=(--set "${BENCH_TYPE}.devices=${READINGS_DEVICE_COUNT}")
          fi
          if [ -n "$READING_COUNT" ]; then
            HELM_ARGS+=(--set "${BENCH_TYPE}.readings=${READING_COUNT}")
          fi
          if [ -n "$BENCHMARK_TIMEOUT" ]; then
            HELM_ARGS+=(--set "timeout=${BENCHMARK_TIMEOUT}")
          fi
          if [ -n "$CLICKBENCH_SIZE" ]; then
            HELM_ARGS+=(--set "clickbench.size=${CLICKBENCH_SIZE}")
          fi
          if [ -n "$CLICKBENCH_LIMIT" ]; then
            HELM_ARGS+=(--set "clickbench.limit=${CLICKBENCH_LIMIT}")
          fi
          if [ -n "$TSBS_DEVICE_COUNT" ]; then
            HELM_ARGS+=(--set "tsbs-iot.devices=${TSBS_DEVICE_COUNT}")
          fi
          if [ -n "$TSBS_QUERY_ITERATIONS" ]; then
            HELM_ARGS+=(--set "tsbs-iot.queryIterations=${TSBS_QUERY_ITERATIONS}")
          fi
          if [ -n "$TSBS_WORKERS" ]; then
            HELM_ARGS+=(--set "tsbs-iot.workers=${TSBS_WORKERS}")
          fi
          if [ -n "$TSBS_BURN_IN" ]; then
            HELM_ARGS+=(--set "tsbs-iot.burnIn=${TSBS_BURN_IN}")
          fi
          if [ -n "$INGEST_TX_OVERHEAD_DOC_COUNT" ]; then
            HELM_ARGS+=(--set "ingestTxOverhead.docCount=${INGEST_TX_OVERHEAD_DOC_COUNT}")
          fi
          if [ -n "$INGEST_TX_OVERHEAD_BATCH_SIZES" ]; then
            # Escape commas for Helm --set parsing
            ESCAPED_BATCH_SIZES="${INGEST_TX_OVERHEAD_BATCH_SIZES//,/\\,}"
            HELM_ARGS+=(--set "ingestTxOverhead.batchSizes=${ESCAPED_BATCH_SIZES}")
          fi
          if [ -n "$PATCH_DOC_COUNT" ]; then
            HELM_ARGS+=(--set "patch.docCount=${PATCH_DOC_COUNT}")
          fi
          if [ -n "$PATCH_PATCH_COUNT" ]; then
            HELM_ARGS+=(--set "patch.patchCount=${PATCH_PATCH_COUNT}")
          fi
          if [ -n "$PRODUCTS_LIMIT" ]; then
            HELM_ARGS+=(--set "products.limit=${PRODUCTS_LIMIT}")
          fi
          if [ -n "$TS_DEVICES_SIZE" ]; then
            HELM_ARGS+=(--set "ts-devices.size=${TS_DEVICES_SIZE}")
          fi
          if [ -n "$FUSION_DEVICE_COUNT" ]; then
            HELM_ARGS+=(--set "fusion.devices=${FUSION_DEVICE_COUNT}")
          fi
          if [ -n "$FUSION_READING_COUNT" ]; then
            HELM_ARGS+=(--set "fusion.readings=${FUSION_READING_COUNT}")
          fi
          if [ -n "$FUSION_DURATION" ]; then
            HELM_ARGS+=(--set "fusion.duration=${FUSION_DURATION}")
          fi
          if [ -n "$FUSION_THREADS" ]; then
            HELM_ARGS+=(--set "fusion.threads=${FUSION_THREADS}")
          fi
          if [ "$RUN_QUEUE" = "true" ]; then
            HELM_ARGS+=(--set "providerConfig.env.RUN_QUEUE=true")
          fi

          helm upgrade --install "xtdb-benchmark" ./modules/bench/cloud/helm "${HELM_ARGS[@]}"

      - name: Check for Immediate Benchmark Failure
        id: immediate-check
        if: steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress'
        env:
          SLACK_LABEL: ${{ env.SLACK_LABEL }}
        run: |
          set -euo pipefail
          echo "immediate_failure=false" >> "$GITHUB_OUTPUT"

          # Run the check (exits non-zero on failure)
          result=$(./modules/bench/cloud/scripts/tasks check-immediate-failure 2>&1) || {
            echo "immediate_failure=true" >> "$GITHUB_OUTPUT"
            echo "::group::check-immediate-failure output"
            echo "$result"
            echo "::endgroup::"
            pod_name=$(echo "$result" | jq -r '.podName // empty' 2>/dev/null || true)
            if [ -n "$pod_name" ]; then
              echo "immediate_failure_pod=${pod_name}" >> "$GITHUB_OUTPUT"
              # Save log preview to file
              log_preview=$(echo "$result" | jq -r '.logPreview // empty' 2>/dev/null || true)
              if [ -n "$log_preview" ]; then
                failure_log_path="$RUNNER_TEMP/immediate-failure-${pod_name}.log"
                echo "$log_preview" | head -n 60 > "$failure_log_path"
                echo "immediate_failure_log_path=${failure_log_path}" >> "$GITHUB_OUTPUT"
              fi
            fi
            # Check if cleanup was already triggered
            cleanup_triggered=$(echo "$result" | jq -r '.cleanupTriggered // false' 2>/dev/null || echo "false")
            if [ "$cleanup_triggered" != "true" ]; then
              echo "No cleanup workflow trigger detected; invoking clear-bench script." >&2
              ./modules/bench/cloud/clear-bench.sh azure || true
            fi
            exit 1
          }

          echo "No immediate benchmark pod failures detected."

      - name: Remove Monitoring Stack on Failure
        if: failure() && (steps.check-deployment.outcome != 'success' || steps.inspect-deployment.outputs.deployment_status != 'in_progress')
        run: |
          if helm status monitoring -n monitoring >/dev/null 2>&1; then
            echo "Uninstalling monitoring stack due to benchmark failure..."
            helm uninstall monitoring -n monitoring || true
            # Attempt to delete the namespace (ignore if it contains other resources or is already gone)
            kubectl delete namespace monitoring --ignore-not-found=true || true
          else
            echo "Monitoring stack not found; skipping."
          fi

      - name: Capture Full Pod Logs on Failure
        id: capture-logs
        if: failure() && steps.immediate-check.outputs.immediate_failure == 'true'
        env:
          POD_NAME: ${{ steps.immediate-check.outputs.immediate_failure_pod }}
          BENCH_TYPE: ${{ inputs.bench_type }}
        run: |
          set -euo pipefail
          LOGS_DIR="$RUNNER_TEMP/benchmark-logs"
          mkdir -p "$LOGS_DIR"
          echo "logs_dir=${LOGS_DIR}" >> "$GITHUB_OUTPUT"

          if [ -n "$POD_NAME" ]; then
            echo "Fetching full logs for pod: $POD_NAME"
            # Get all container logs (main + init containers)
            kubectl logs "$POD_NAME" -n cloud-benchmark --all-containers=true --ignore-errors > "$LOGS_DIR/${BENCH_TYPE}-${POD_NAME}.log" 2>&1 || true

            # Also get pod describe for debugging
            kubectl describe pod "$POD_NAME" -n cloud-benchmark > "$LOGS_DIR/${BENCH_TYPE}-${POD_NAME}-describe.txt" 2>&1 || true

            # Get events for the namespace
            kubectl get events -n cloud-benchmark --sort-by='.lastTimestamp' > "$LOGS_DIR/${BENCH_TYPE}-events.txt" 2>&1 || true

            echo "Captured logs:"
            ls -la "$LOGS_DIR"
          else
            echo "No pod name available for log capture"
          fi

      - name: Upload Failure Logs
        if: failure() && steps.capture-logs.outputs.logs_dir != ''
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v4
        with:
          name: benchmark-failure-logs-${{ inputs.bench_type }}-${{ github.run_id }}
          path: ${{ steps.capture-logs.outputs.logs_dir }}
          retention-days: 14

      - name: Compose Slack Message
        id: compose
        if: always()
        env:
          SLACK_LABEL: ${{ env.SLACK_LABEL }}
          SCALE_FACTOR: ${{ env.SCALE_FACTOR }}
          DURATION: ${{ inputs.duration }}
          THREADS: ${{ inputs.threads }}
          NODE_COUNT: ${{ inputs.node_count }}
          READINGS_DEVICE_COUNT: ${{ inputs.readings_device_count }}
          READING_COUNT: ${{ inputs.reading_count }}
          INGEST_TX_OVERHEAD_DOC_COUNT: ${{ inputs.ingest_tx_overhead_doc_count }}
          INGEST_TX_OVERHEAD_BATCH_SIZES: ${{ inputs.ingest_tx_overhead_batch_sizes }}
          PATCH_DOC_COUNT: ${{ inputs.patch_doc_count }}
          PATCH_PATCH_COUNT: ${{ inputs.patch_patch_count }}
          PRODUCTS_LIMIT: ${{ inputs.products_limit }}
          TS_DEVICES_SIZE: ${{ inputs.ts_devices_size }}
          CLICKBENCH_SIZE: ${{ inputs.clickbench_size }}
          CLICKBENCH_LIMIT: ${{ inputs.clickbench_limit }}
          TSBS_DEVICE_COUNT: ${{ inputs.tsbs_device_count }}
          TSBS_QUERY_ITERATIONS: ${{ inputs.tsbs_query_iterations }}
          TSBS_WORKERS: ${{ inputs.tsbs_workers }}
          TSBS_BURN_IN: ${{ inputs.tsbs_burn_in }}
          FUSION_DEVICE_COUNT: ${{ inputs.fusion_device_count }}
          FUSION_READING_COUNT: ${{ inputs.fusion_reading_count }}
          FUSION_DURATION: ${{ inputs.fusion_duration }}
          FUSION_THREADS: ${{ inputs.fusion_threads }}
          GIT_SHA: ${{ github.sha }}
          GIT_BRANCH: ${{ inputs.branch }}
          REPO_URL: ${{ github.server_url }}/${{ github.repository }}
          IMAGE_OWNER: ${{ inputs.image_owner || github.repository_owner }}
          IMAGE_TAG: ${{ needs.build-benchmark-image.outputs.image_tag }}
        run: |
          set -euo pipefail

          SHORT_SHA="${GIT_SHA:0:7}"
          GIT_INFO="Branch: <${REPO_URL}/tree/${GIT_BRANCH}|\`${GIT_BRANCH}\`> | Commit: <${REPO_URL}/commit/${GIT_SHA}|\`${SHORT_SHA}\`>"
          IMAGE="ghcr.io/${IMAGE_OWNER}/xtdb-bench:${IMAGE_TAG}"
          # Strip ghcr.io/ prefix for display
          IMAGE_DISPLAY="${IMAGE#ghcr.io/}"
          IMAGE_INFO="Image: <https://github.com/${IMAGE_OWNER}/xtdb-bench/pkgs/container/xtdb-bench|\`${IMAGE_DISPLAY}\`>"

          # Build config info
          CONFIG_PARTS=()
          [ -n "$SCALE_FACTOR" ] && CONFIG_PARTS+=("Scale Factor: ${SCALE_FACTOR}")
          [ -n "$DURATION" ] && CONFIG_PARTS+=("Duration: ${DURATION}")
          [ -n "$THREADS" ] && CONFIG_PARTS+=("Threads: ${THREADS}")
          [ -n "$NODE_COUNT" ] && CONFIG_PARTS+=("Nodes: ${NODE_COUNT}")
          [ -n "$READINGS_DEVICE_COUNT" ] && CONFIG_PARTS+=("Devices: ${READINGS_DEVICE_COUNT}")
          [ -n "$READING_COUNT" ] && CONFIG_PARTS+=("Readings: ${READING_COUNT}")
          [ -n "$INGEST_TX_OVERHEAD_DOC_COUNT" ] && CONFIG_PARTS+=("Docs: ${INGEST_TX_OVERHEAD_DOC_COUNT}")
          [ -n "$INGEST_TX_OVERHEAD_BATCH_SIZES" ] && CONFIG_PARTS+=("Batch Sizes: ${INGEST_TX_OVERHEAD_BATCH_SIZES}")
          [ -n "$PATCH_DOC_COUNT" ] && CONFIG_PARTS+=("Docs: ${PATCH_DOC_COUNT}")
          [ -n "$PATCH_PATCH_COUNT" ] && CONFIG_PARTS+=("Patches: ${PATCH_PATCH_COUNT}")
          [ -n "$CLICKBENCH_SIZE" ] && CONFIG_PARTS+=("Size: ${CLICKBENCH_SIZE}")
          [ -n "$CLICKBENCH_LIMIT" ] && CONFIG_PARTS+=("Limit: ${CLICKBENCH_LIMIT}")
          [ -n "$TSBS_DEVICE_COUNT" ] && CONFIG_PARTS+=("Devices: ${TSBS_DEVICE_COUNT}")
          [ -n "$TSBS_QUERY_ITERATIONS" ] && CONFIG_PARTS+=("Query Iterations: ${TSBS_QUERY_ITERATIONS}")
          [ -n "$TSBS_WORKERS" ] && CONFIG_PARTS+=("Workers: ${TSBS_WORKERS}")
          [ -n "$TSBS_BURN_IN" ] && CONFIG_PARTS+=("Burn-In: ${TSBS_BURN_IN}")
          [ -n "$PRODUCTS_LIMIT" ] && CONFIG_PARTS+=("Limit: ${PRODUCTS_LIMIT}")
          [ -n "$TS_DEVICES_SIZE" ] && CONFIG_PARTS+=("Size: ${TS_DEVICES_SIZE}")
          [ -n "$FUSION_DEVICE_COUNT" ] && CONFIG_PARTS+=("Devices: ${FUSION_DEVICE_COUNT}")
          [ -n "$FUSION_READING_COUNT" ] && CONFIG_PARTS+=("Readings: ${FUSION_READING_COUNT}")
          [ -n "$FUSION_DURATION" ] && CONFIG_PARTS+=("Duration: ${FUSION_DURATION}")
          [ -n "$FUSION_THREADS" ] && CONFIG_PARTS+=("Threads: ${FUSION_THREADS}")
          # Join with " | "
          if [ ${#CONFIG_PARTS[@]} -gt 0 ]; then
            CONFIG_INFO=$(printf " | %s" "${CONFIG_PARTS[@]}")
            CONFIG_INFO="${CONFIG_INFO:3}" # Strip leading " | "
          else
            CONFIG_INFO=""
          fi

          if [ "${{ steps.check-deployment.outcome }}" = "success" ] && [ "${{ steps.inspect-deployment.outputs.deployment_status }}" = "in_progress" ]; then
            {
              echo 'msg<<EOF_MSG'
              echo "*${SLACK_LABEL} Benchmark* :information_source: Skipped â€” existing deployment found"
              echo "${GIT_INFO}"
              echo "${IMAGE_INFO}"
              echo 'EOF_MSG'
            } >> "$GITHUB_OUTPUT"
          elif [ "${{ steps.immediate-check.outputs.immediate_failure }}" = "true" ]; then
            POD_NAME='${{ steps.immediate-check.outputs.immediate_failure_pod }}'
            LOG_PATH='${{ steps.immediate-check.outputs.immediate_failure_log_path }}'
            MSG_FILE="$RUNNER_TEMP/slack-immediate-failure.txt"
            : > "$MSG_FILE"
            {
              printf '*%s Benchmark* :x: Failed during startup (pod: `%s`)\n' "${SLACK_LABEL}" "${POD_NAME:-unknown}"
              echo "${GIT_INFO}"
              echo "${IMAGE_INFO}"
              if [ -n "$LOG_PATH" ] && [ -f "$LOG_PATH" ]; then
                echo >> "$MSG_FILE"
                echo '```' >> "$MSG_FILE"
                cat "$LOG_PATH" >> "$MSG_FILE"
                echo '```' >> "$MSG_FILE"
              fi
            } >> "$MSG_FILE"

            {
              echo 'msg<<EOF_MSG'
              cat "$MSG_FILE"
              echo 'EOF_MSG'
            } >> "$GITHUB_OUTPUT"
          else
            {
              echo 'msg<<EOF_MSG'
              if [ -n "${CONFIG_INFO}" ]; then
                echo "*${SLACK_LABEL} Benchmark* :rocket: Started (${CONFIG_INFO})"
              else
                echo "*${SLACK_LABEL} Benchmark* :rocket: Started"
              fi
              echo "${GIT_INFO}"
              echo "${IMAGE_INFO}"
              echo 'EOF_MSG'
            } >> "$GITHUB_OUTPUT"
          fi

      - name: Prepare Slack Message
        if: always()
        run: |
          jq -n --arg channel "$SLACK_CHANNEL" --arg text "$SLACK_MESSAGE" \
            '{channel: $channel, text: $text}' > slack-message.json
        env:
          SLACK_CHANNEL: ${{ secrets.BENCHMARK_SLACK_CHANNEL_ID }}
          SLACK_MESSAGE: ${{ steps.compose.outputs.msg }}

      - name: Post Slack Notification
        if: always()
        uses: slackapi/slack-github-action@91efab103c0de0a537f72a35f6b8cda0ee76bf0a # v2.1.1
        with:
          method: chat.postMessage
          token: ${{ secrets.BENCHMARK_SLACK_BOT_TOKEN }}
          payload-file-path: ./slack-message.json
