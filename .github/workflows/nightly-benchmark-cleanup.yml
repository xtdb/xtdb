name: Nightly Benchmark Cleanup
run-name: >-
  Benchmark Cleanup: ${{ inputs.benchType == 'tpch' && 'TPC-H' || inputs.benchType == 'auctionmark' && 'AuctionMark' || inputs.benchType == 'readings' && 'Readings' || inputs.benchType == 'yakbench' && 'Yakbench' || inputs.benchType }}

on:
  workflow_dispatch:
    inputs:
      benchType:
        description: 'Type of benchmark to clean up'
        required: true
        type: choice
        options:
          - tpch
          - auctionmark
          - readings
          - yakbench
      status:
        description: 'Status of the benchmark'
        required: true
        type: choice
        options:
          - success
          - failure
      nodeId:
        description: 'Node ID of the benchmark'
        required: true
        type: string

permissions:
  id-token: write
  contents: read
  actions: write

concurrency:
  group: cleanup-benchmarks
  cancel-in-progress: false

jobs:
  cleanup:
    runs-on: ubuntu-latest
    env:
      GRAFANA_ADMIN_PASSWORD: ${{ secrets.BENCHMARK_GRAFANA_ADMIN_PASSWORD }}
    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5

      - name: Install Babashka
        run: |
          set -euo pipefail
          BABASHKA_VERSION=1.12.209
          curl -sSfL "https://github.com/babashka/babashka/releases/download/v${BABASHKA_VERSION}/babashka-${BABASHKA_VERSION}-linux-amd64.tar.gz" -o /tmp/babashka.tar.gz
          sudo tar -xzf /tmp/babashka.tar.gz -C /usr/local/bin bb
          rm -f /tmp/babashka.tar.gz
          bb --version

      - name: Azure CLI Login
        uses: azure/login@a457da9ea143d694b1b9c7c869ebb04ebe844ef5 # v2
        with:
          client-id: ${{ secrets.BENCHMARK_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.BENCHMARK_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.BENCHMARK_AZURE_SUBSCRIPTION_ID }}

      - name: Acquire Kubernetes Configuration
        run: |
          az aks get-credentials --resource-group cloud-benchmark-resources --name xtdb-bench-cluster

      - name: Check Deployment
        id: check-deployment
        continue-on-error: true
        run: |
          # exit 0 if exists, non-zero if not
          helm status xtdb-benchmark -n cloud-benchmark >/dev/null 2>&1

      - name: Cancel workflow (no deployment found)
        if: steps.check-deployment.outcome != 'success'
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            core.notice('No benchmark deployment found. Cancelling cleanup workflow run.');
            await github.rest.actions.cancelWorkflowRun({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId,
            });

      - name: Stop job after cancellation
        if: steps.check-deployment.outcome != 'success'
        run: |
          echo "Cleanup workflow cancellation requested due to missing deployment. Stopping job."
          exit 0

      - name: Wait for Benchmark Job and Pods to Complete
        if: steps.check-deployment.outcome == 'success'
        run: |
          set -euo pipefail
          JOB="${{ inputs.benchType }}"
          NS="cloud-benchmark"

          # If all benchmark pods are already in a terminal error state, don't block waiting
          sel='-l app.kubernetes.io/name=xtdb,app.kubernetes.io/component=benchmark'
          failed_cnt=$(kubectl get pods -n "$NS" $sel --field-selector=status.phase=Failed --no-headers 2>/dev/null | wc -l | tr -d ' ')
          nonterminal_cnt=$(kubectl get pods -n "$NS" $sel --field-selector=status.phase!=Succeeded,status.phase!=Failed --no-headers 2>/dev/null | wc -l | tr -d ' ')
          if [ "${nonterminal_cnt:-0}" = "0" ] && [ "${failed_cnt:-0}" -gt 0 ]; then
            echo "All benchmark pods are in Failed state; skipping waits."
          else
          # Wait for the benchmark Job (tpch/auctionmark/readings/yakbench) to complete if present
          if kubectl get job "$JOB" -n "$NS" >/dev/null 2>&1; then
            echo "Polling job/$JOB until Complete or Failed..."
            # Poll up to ~2h (720 * 10s)
            for i in $(seq 1 720); do
              COMPLETE=$(kubectl get job "$JOB" -n "$NS" -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' || true)
              FAILED=$(kubectl get job "$JOB" -n "$NS" -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' || true)
              if [ "$COMPLETE" = "True" ]; then
                echo "job/$JOB Complete"
                break
              fi
              if [ "$FAILED" = "True" ]; then
                echo "job/$JOB Failed"
                break
              fi

              # Optional: stop early if all benchmark pods are terminal
              remaining=$(kubectl get pods -n "$NS" \
                -l app.kubernetes.io/name=xtdb,app.kubernetes.io/component=benchmark \
                --field-selector=status.phase!=Succeeded,status.phase!=Failed \
                --no-headers 2>/dev/null | wc -l | tr -d ' ')
              if [ "${remaining:-0}" = "0" ]; then
                echo "All benchmark pods are terminal; stopping job wait"
                break
              fi

              sleep 10
            done
          else
            echo "job/$JOB not found in namespace $NS; continuing"
          fi

          # Gracefully wait for benchmark pods to finish terminating
          # (pods created by the benchmark Job under app.kubernetes.io/name=xtdb & component=benchmark)
          for i in $(seq 1 30); do
            remaining=$(kubectl get pods -n "$NS" \
              -l app.kubernetes.io/name=xtdb,app.kubernetes.io/component=benchmark \
              --field-selector=status.phase!=Succeeded,status.phase!=Failed \
              --no-headers 2>/dev/null | wc -l | tr -d ' ')
            if [ "${remaining:-0}" = "0" ]; then
              echo "All benchmark pods completed/terminated"
              break
            fi
            echo "Waiting for benchmark pods to complete... ($i) remaining=$remaining"
            sleep 4
          done
          fi

      - name: Determine Benchmark Status
        id: derive
        if: steps.check-deployment.outcome == 'success'
        run: |
          set -euo pipefail
          JOB="${{ inputs.benchType }}"
          NS="cloud-benchmark"

          STATUS="unknown"
          if kubectl get job "$JOB" -n "$NS" >/dev/null 2>&1; then
            COMPLETE=$(kubectl get job "$JOB" -n "$NS" -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' || true)
            FAILED=$(kubectl get job "$JOB" -n "$NS" -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' || true)
            if [ "$FAILED" = "True" ]; then STATUS="failure"; fi
            if [ "$COMPLETE" = "True" ]; then STATUS="success"; fi
          else
            # If the job isn't present, infer from pods: any Failed pod => failure, any Succeeded pod => success
            sel='-l app.kubernetes.io/name=xtdb,app.kubernetes.io/component=benchmark'
            failed_cnt=$(kubectl get pods -n "$NS" $sel --field-selector=status.phase=Failed --no-headers 2>/dev/null | wc -l | tr -d ' ')
            succeeded_cnt=$(kubectl get pods -n "$NS" $sel --field-selector=status.phase=Succeeded --no-headers 2>/dev/null | wc -l | tr -d ' ')
            if [ "${failed_cnt:-0}" -gt 0 ]; then STATUS="failure";
            elif [ "${succeeded_cnt:-0}" -gt 0 ]; then STATUS="success";
            fi
          fi

          echo "bench_status=${STATUS}" >> "$GITHUB_OUTPUT"
          echo "Derived benchmark status: ${STATUS}"

      - name: Capture Benchmark Logs (All Pods)
        if: steps.check-deployment.outcome == 'success'
        id: logs
        run: |
          set -euo pipefail
          NS="cloud-benchmark"
          JOB_NAME="${{ inputs.benchType }}"

          mkdir -p pod-logs
          pods=$(kubectl get pods -n "$NS" -l job-name="$JOB_NAME" -o jsonpath='{.items[*].metadata.name}' || true)

          # Fallback selector (in case job-name label is missing): select XTDB benchmark pods by component/name
          if [ -z "$pods" ]; then
            pods=$(kubectl get pods -n "$NS" -l app.kubernetes.io/name=xtdb,app.kubernetes.io/component=benchmark -o jsonpath='{.items[*].metadata.name}' || true)
          fi

          # Download logs for each pod with a small retry loop
          total_ms=0
          count_ms=0
          primary_log=""
          for pod in $pods; do
            attempts=0
            # Try primary container logs explicitly, with retries (handles pods that are still terminating)
            until kubectl logs "$pod" -n "$NS" -c xtdb-node > "pod-logs/$pod.log" 2>/dev/null || \
                  kubectl logs "$pod" -n "$NS" > "pod-logs/$pod.log" 2>/dev/null || \
                  [ $attempts -ge 12 ]; do
              attempts=$((attempts+1))
              echo "Waiting for logs from $pod (attempt $attempts)..."
              sleep 5
            done

            # If empty or unavailable, try previous logs (in case of restarts)
            if [ ! -s "pod-logs/$pod.log" ]; then
              kubectl logs "$pod" -n "$NS" -c xtdb-node --previous > "pod-logs/$pod.previous.log" 2>/dev/null || \
              kubectl logs "$pod" -n "$NS" --previous > "pod-logs/$pod.previous.log" 2>/dev/null || true
            fi

            # If still nothing, capture describe for diagnostics
            if [ ! -s "pod-logs/$pod.log" ] && [ ! -s "pod-logs/$pod.previous.log" ]; then
              kubectl describe pod "$pod" -n "$NS" > "pod-logs/$pod.describe.txt" 2>/dev/null || true
            fi
            # Choose a primary log for convenience (use previous if primary absent)
            if [ -z "$primary_log" ]; then
              src_for_primary=""
              if [ -s "pod-logs/$pod.log" ]; then src_for_primary="pod-logs/$pod.log"; fi
              if [ -z "$src_for_primary" ] && [ -s "pod-logs/$pod.previous.log" ]; then src_for_primary="pod-logs/$pod.previous.log"; fi
              if [ -n "$src_for_primary" ]; then
                cp "$src_for_primary" benchmark.log || true
                primary_log="$pod"
              fi
            fi
            if [ -s "pod-logs/$pod.log" ] || [ -s "pod-logs/$pod.previous.log" ]; then
              # Extract last time-taken-ms in this pod's log
              src_log="pod-logs/$pod.log"
              [ -s "pod-logs/$pod.log" ] || src_log="pod-logs/$pod.previous.log"
              pod_ms=$(grep -o '"time-taken-ms":[0-9]\+' "$src_log" | tail -1 | cut -d: -f2 || true)
              if [ -n "$pod_ms" ]; then
                total_ms=$((total_ms + pod_ms))
                count_ms=$((count_ms + 1))
              fi
              # Capture params EDN once (supports tpch, auctionmark, or yakbench namespaces in loggers)
              if [ -z "${{ steps.logs.outputs.params_edn || '' }}" ]; then
                params_line=$(grep -m1 -E 'INFO[[:space:]]+xtdb\.bench\.(tpch|auctionmark|yakbench)[[:space:]]+-[[:space:]]+\{.*\}' "$src_log" || true)
                if [ -n "$params_line" ]; then
                  params_edn=$(echo "$params_line" | sed -E 's/.*INFO[[:space:]]+xtdb\.bench\.(tpch|auctionmark|yakbench)[[:space:]]+-[[:space:]]+(\{.*\}).*/\2/')
                  {
                    echo 'params_edn<<EOF'
                    echo "$params_edn"
                    echo 'EOF'
                  } >> "$GITHUB_OUTPUT"
                  sf=$(echo "$params_edn" | sed -nE 's/.*:scale-factor[[:space:]]+([0-9.]+).*/\1/p' || true)
                  if [ -n "${sf}" ]; then
                    echo "SCALE_FACTOR=${sf}" >> "$GITHUB_ENV"
                  fi
                fi
              fi
            fi
          done

          # If we never picked a primary, try the nodeId the job passed us
          if [ -z "$primary_log" ]; then
            if kubectl logs "${{ inputs.nodeId }}" -n "$NS" > benchmark.log 2>/dev/null; then
              primary_log="${{ inputs.nodeId }}"
            fi
          fi

          # Compute average time if we have any samples
          if [ "$count_ms" -gt 0 ]; then
            avg_ms=$((total_ms / count_ms))
            echo "time_taken=$avg_ms" >> "$GITHUB_OUTPUT"

            total_seconds=$((avg_ms / 1000))
            rem_ms=$((avg_ms % 1000))
            hours=$((total_seconds / 3600))
            minutes=$(((total_seconds % 3600) / 60))
            seconds=$((total_seconds % 60))
            sec_str="$seconds"
            if [ "$rem_ms" -gt 0 ]; then
              ms_padded=$(printf "%03d" "$rem_ms")
              sec_str="${seconds}.${ms_padded}"
            fi
            duration="PT"
            if [ "$hours" -gt 0 ]; then duration="${duration}${hours}H"; fi
            if [ "$minutes" -gt 0 ]; then duration="${duration}${minutes}M"; fi
            if [ "$seconds" -gt 0 ] || [ "$rem_ms" -gt 0 ] || [ "$duration" = "PT" ]; then
              duration="${duration}${sec_str}S"
            fi
            echo "time_taken_iso=$duration" >> "$GITHUB_OUTPUT"
          fi

      - name: Set Timestamp
        id: timestamp
        run: |
          echo "value=$(date -u +"%Y-%m-%d-%H-%M-%S")" >> "$GITHUB_OUTPUT"
          echo "date=$(date -u +"%Y/%m/%d")" >> "$GITHUB_OUTPUT"

      - name: Upload Benchmark Logs
        id: upload-logs
        if: steps.check-deployment.outcome == 'success' && hashFiles('benchmark.log', 'pod-logs/*.log') != ''
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5.0.0
        with:
          name: benchmark-logs-${{ inputs.benchType }}-${{ steps.timestamp.outputs.value }}-${{ inputs.nodeId }}-${{ steps.derive.outputs.bench_status || inputs.status }}
          path: |
            pod-logs/*.log
          retention-days: 7

      - name: Generate metrics.json
        if: steps.check-deployment.outcome == 'success' && steps.derive.outputs.bench_status == 'success' && steps.logs.outputs.time_taken != ''
        env:
          BENCH_TYPE: ${{ inputs.benchType }}
          GIT_SHA: ${{ github.sha }}
          REPO: ${{ github.repository }}
          NODE_ID: ${{ inputs.nodeId }}
          TIME_TAKEN: ${{ steps.logs.outputs.time_taken }}
          # Optional context if available (left blank by default)
          SCALE_FACTOR: ${{ env.SCALE_FACTOR }}
          CONCURRENCY: ${{ env.CONCURRENCY }}
        run: |
          set -euo pipefail

          TS_NOW=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          # Defaults for params (numeric fields will be omitted if empty)
          SCALE_FACTOR_VAL=${SCALE_FACTOR:-}
          CONCURRENCY_VAL=${CONCURRENCY:-}

          # Build params fragment dynamically
          PARAMS_FIELDS=()
          if [ -n "${SCALE_FACTOR_VAL}" ]; then PARAMS_FIELDS+=("\"scaleFactor\": ${SCALE_FACTOR_VAL}"); fi
          if [ -n "${CONCURRENCY_VAL}" ]; then PARAMS_FIELDS+=("\"concurrency\": ${CONCURRENCY_VAL}"); fi
          PARAMS_JSON="$(IFS=, ; echo "${PARAMS_FIELDS[*]}")"

          cat > metrics.json <<JSON
          [
            {
              "run_id": "${GITHUB_RUN_ID}",
              "git_sha": "${GIT_SHA}",
              "repo": "${REPO}",
              "benchmark": "${BENCH_TYPE}",
              "step": "overall",
              "node_id": "${NODE_ID}",
              "metric": "duration_ms",
              "value": ${TIME_TAKEN},
              "unit": "ms",
              "ts": "${TS_NOW}",
              "params": { ${PARAMS_JSON} }
            }
          ]
          JSON
          echo "Generated metrics.json:" && cat metrics.json

      - name: Azure CLI Re-Login (refresh OIDC)
        uses: azure/login@a457da9ea143d694b1b9c7c869ebb04ebe844ef5 # v2
        with:
          client-id: ${{ secrets.BENCHMARK_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.BENCHMARK_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.BENCHMARK_AZURE_SUBSCRIPTION_ID }}

      - name: Post metrics to Azure Monitor
        if: steps.check-deployment.outcome == 'success' && steps.derive.outputs.bench_status == 'success' && steps.logs.outputs.time_taken != ''
        env:
          DCR_IMMUTABLE_ID: ${{ secrets.BENCHMARK_DCR_IMMUTABLE_ID }}
          DCE_ENDPOINT: ${{ secrets.BENCHMARK_DCE_ENDPOINT }}
          DCR_STREAM: ${{ secrets.BENCHMARK_DCR_STREAM }}
        run: |
          set -euo pipefail
          # Validate JSON payload
          jq -e . metrics.json >/dev/null
          echo "metrics.json is valid. Size and preview:"
          ls -l metrics.json
          cat metrics.json

          # Acquire an access token for the Azure Monitor resource
          ACCESS_TOKEN=$(az account get-access-token --resource https://monitor.azure.com --query accessToken -o tsv)
          if [ -z "${ACCESS_TOKEN}" ]; then
            echo "Failed to acquire Azure Monitor access token" >&2
            exit 1
          fi

          URL="${DCE_ENDPOINT%/}/dataCollectionRules/${DCR_IMMUTABLE_ID}/streams/${DCR_STREAM}?api-version=2023-01-01"
          echo "POST -> $URL"

          # Send the request (payload must be a JSON array of records)
          curl -sS -v -X POST "$URL" \
            -H "Authorization: Bearer ${ACCESS_TOKEN}" \
            -H "Content-Type: application/json" \
            --data-binary @metrics.json

      - name: Compute Grafana Time Range
        id: grafana-time
        if: steps.check-deployment.outcome == 'success'
        run: |
          set -euo pipefail
          FROM_PARAM="now-2h"
          TO_PARAM="now"
          if [ -f benchmark.log ]; then
            # Extract first timestamp like HH:MM:SS.mmm
            start_ts=$(grep -m1 -oE '^[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]{3}' benchmark.log || true)
            if [ -n "$start_ts" ]; then
              start_hms=${start_ts%.*}
              start_ms=${start_ts#*.}
              today=$(date -u +%Y-%m-%d)
              # Candidates: today and yesterday (to handle jobs crossing midnight UTC)
              cand1=$(date -u -d "$today $start_hms" +%s 2>/dev/null || echo "")
              yesterday=$(date -u -d "$today - 1 day" +%Y-%m-%d)
              cand2=$(date -u -d "$yesterday $start_hms" +%s 2>/dev/null || echo "")
              now_s=$(date -u +%s)
              pick=""
              if [ -n "$cand1" ] && [ "$cand1" -le "$now_s" ]; then pick="$cand1"; fi
              if [ -n "$cand2" ] && [ "$cand2" -le "$now_s" ]; then
                if [ -z "$pick" ] || [ $((now_s - cand2)) -lt $((now_s - pick)) ]; then pick="$cand2"; fi
              fi
              if [ -n "$pick" ]; then
                # Ensure ms parsed as decimal even with leading zeros
                FROM_PARAM=$((pick * 1000 + 10#${start_ms}))
                TO_PARAM="$(date -u +%s000)"
              fi
            fi
          fi
          echo "from=$FROM_PARAM" >> "$GITHUB_OUTPUT"
          echo "to=$TO_PARAM" >> "$GITHUB_OUTPUT"

      - name: Port-forward Grafana (background)
        if: steps.check-deployment.outcome == 'success'
        run: |
          # Start port-forward in background
          kubectl -n monitoring port-forward svc/monitoring-grafana 3000:80 >/tmp/grafana-pf.log 2>&1 &
          echo $! > /tmp/grafana-pf.pid
          # Wait for Grafana to be reachable
          for i in {1..30}; do
            if curl -sSf http://localhost:3000/api/health > /dev/null; then
              echo "Grafana is reachable"
              break
            fi
            echo "Waiting for Grafana port-forward... ($i)"
            sleep 2
          done

      - name: Render Cluster Monitoring Panels
        if: steps.check-deployment.outcome == 'success'
        run: |
          set -euo pipefail
          mkdir -p grafana-panels
          DASHBOARD_UID="aeews5bgs0zk0b"
          FROM="${{ steps.grafana-time.outputs.from || 'now-2h' }}"
          TO="${{ steps.grafana-time.outputs.to || 'now' }}"
          WIDTH=1600
          HEIGHT=900
          SLUG="xtdb-monitoring-dashboard"

          # Declare panels: [title]=panelId
          declare -A PANELS=(
            ["# Nodes"]=1
            ["Transaction Lag"]=3
            ["Max Transaction Latency (Over Time)"]=5
            ["Max Query Latency (Over Time)"]=6
            ["Cluster Transactions"]=8
            ["Cluster Transactions Latencies"]=7
            ["Cluster Transaction Errors"]=22
            ["Cluster Queries"]=9
            ["Cluster Query Latencies"]=10
            ["Cluster Query Errors"]=20
            ["Cluster PGWire Active Connections"]=23
          )

          # Simple slugify for file names
          slugify() {
            echo "$1" | tr '[:upper:]' '[:lower:]' | sed -E 's/[^a-z0-9]+/-/g; s/^-+|-+$//g'
          }

          for title in "${!PANELS[@]}"; do
            panelId="${PANELS[$title]}"
            file="grafana-panels/$(slugify "$title").png"
            url="http://localhost:3000/render/d-solo/${DASHBOARD_UID}/${SLUG}?panelId=${panelId}&orgId=1&from=${FROM}&to=${TO}&width=${WIDTH}&height=${HEIGHT}&tz=UTC"
            echo "Rendering [$title] (panelId=${panelId}) -> $file"
            curl -sS -f -L -u "admin:${GRAFANA_ADMIN_PASSWORD}" -H 'Accept: image/png' "$url" -o "$file" || echo "Failed to render $title"
          done

      - name: Render Node Debugging Panels
        if: steps.check-deployment.outcome == 'success'
        run: |
          set -euo pipefail
          # Determine all XTDB node ids (pod names) to use for var-xtdbnode
          PODS=$(kubectl get pods -n cloud-benchmark -l app.kubernetes.io/component=benchmark,app.kubernetes.io/name=xtdb -o jsonpath='{.items[*].metadata.name}' || true)
          if [ -z "$PODS" ]; then
            # Fallback via plain text parsing
            PODS=$(kubectl get pods -n cloud-benchmark -l app.kubernetes.io/component=benchmark,app.kubernetes.io/name=xtdb -o name | sed 's#.*/##' || true)
          fi
          if [ -z "$PODS" ]; then
            echo "Failed to determine benchmark pods; skipping node debugging panel rendering"
            exit 0
          fi
          echo "Rendering node debugging panels for pods: $PODS"

          mkdir -p grafana-panels
          DASHBOARD_UID="edznf2lfly22o1aa" # XTDB: Node Debugging Dashboard
          FROM="${{ steps.grafana-time.outputs.from || 'now-2h' }}"
          TO="${{ steps.grafana-time.outputs.to || 'now' }}"
          WIDTH=1600
          HEIGHT=900
          SLUG="xtdb-node-debugging"

          declare -A PANELS=(
            ["JVM - Heap Usage (Average)"]=8
            ["JVM - Heap Usage (Max)"]=20
            ["JVM - Buffer Used"]=9
            ["JVM - Live Threads"]=10
            ["Transactions - Count"]=19
            ["Transactions - Timer (Max)"]=17
            ["Transactions - Timer (Quantiles)"]=16
            ["Query - Count"]=18
            ["Query - Timer (Max)"]=15
            ["Query - Timer (Quantiles)"]=14
            ["XTDB - Buffer Pool Allocated Memory"]=24
            ["XTDB - Compactor Allocated Memory"]=3
            ["Netty Allocated Memory"]=38
            ["Direct Allocated Memory"]=40
            ["Memory Cache - Record Batch count"]=26
            ["Buffer Pool - Record Batch requests"]=36
            ["Available Compaction Jobs"]=42
            ["Compaction Job Timer"]=45
          )

          slugify() {
            echo "$1" | tr '[:upper:]' '[:lower:]' | sed -E 's/[^a-z0-9]+/-/g; s/^-+|-+$//g'
          }

          for node in $PODS; do
            for title in "${!PANELS[@]}"; do
              panelId="${PANELS[$title]}"
              file="grafana-panels/${node}-node-$(slugify "$title").png"
              url="http://localhost:3000/render/d-solo/${DASHBOARD_UID}/${SLUG}?panelId=${panelId}&orgId=1&from=${FROM}&to=${TO}&width=${WIDTH}&height=${HEIGHT}&tz=UTC&var-xtdbnode=${node}"
              echo "Rendering [Node Debug] $title (panelId=${panelId}, xtdbnode=${node}) -> $file"
              curl -sS -f -L -u "admin:${GRAFANA_ADMIN_PASSWORD}" -H 'Accept: image/png' "$url" -o "$file" || echo "Failed to render node panel $title for ${node}"
            done
          done

      - name: Upload Grafana Panels
        if: steps.check-deployment.outcome == 'success' && hashFiles('grafana-panels/*.png') != ''
        id: upload-grafana
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5.0.0
        with:
          name: grafana-panels-${{ inputs.benchType }}-${{ steps.timestamp.outputs.value }}-${{ inputs.nodeId }}-${{ inputs.status }}
          path: grafana-panels/*.png
          retention-days: 7

      - name: Stop Grafana Port-forward
        if: always()
        run: |
          if [ -f /tmp/grafana-pf.pid ]; then
            kill "$(cat /tmp/grafana-pf.pid)" 2>/dev/null || true
            rm -f /tmp/grafana-pf.pid
          fi

      - name: Run Cleanup Script
        if: ${{ always() && steps.check-deployment.outcome == 'success' }}
        run: |
          ./modules/bench/cloud/clear-bench.sh azure

      - name: Remove Monitoring Stack
        if: always()
        run: |
          if helm status monitoring -n monitoring >/dev/null 2>&1; then
            echo "Uninstalling monitoring stack..."
            helm uninstall monitoring -n monitoring
            # Attempt to delete the namespace (ignore if it contains other resources or is already gone)
            kubectl delete namespace monitoring --ignore-not-found=true || true
          else
            echo "Monitoring stack not found; skipping."
          fi

      - name: Write Job Summary
        if: always()
        run: |
          set -euo pipefail
          bench="${{ inputs.benchType }}"
          case "$bench" in
            tpch) bench_friendly="TPC-H" ;;
            auctionmark) bench_friendly="AuctionMark" ;;
            readings) bench_friendly="Readings" ;;
            yakbench) bench_friendly="Yakbench" ;;
            *) bench_friendly="$bench" ;;
          esac

          duration_iso="${{ steps.logs.outputs.time_taken_iso || '' }}"
          node_id="${{ inputs.nodeId }}"
          status_input="${{ steps.derive.outputs.bench_status || inputs.status }}"
          repo="${{ github.repository }}"
          run_url="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          scale_factor_val="${{ env.SCALE_FACTOR || '' }}"
          concurrency_val="${{ env.CONCURRENCY || '' }}"

          echo "# Benchmark Summary" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Benchmark**: ${bench_friendly}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Status**: ${status_input}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Node ID**: ${node_id}" >> "$GITHUB_STEP_SUMMARY"
          if [ -n "$duration_iso" ]; then
            echo "- **Run duration**: ${duration_iso}" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ -n "$scale_factor_val" ]; then
            echo "- **Scale factor**: ${scale_factor_val}" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ -n "$concurrency_val" ]; then
            echo "- **Concurrency**: ${concurrency_val}" >> "$GITHUB_STEP_SUMMARY"
          fi
          echo "- **Repository**: ${repo}" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          # Parameters (raw EDN captured, if any)
          if [ -n "${{ steps.logs.outputs.params_edn || '' }}" ]; then
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo "## Raw Parameters (EDN)" >> "$GITHUB_STEP_SUMMARY"
            echo '```edn' >> "$GITHUB_STEP_SUMMARY"
            echo "${{ steps.logs.outputs.params_edn }}" >> "$GITHUB_STEP_SUMMARY"
            echo '```' >> "$GITHUB_STEP_SUMMARY"
          fi

          log_path="$(pwd)/benchmark.log"
          summary="$(cd modules/bench/cloud/scripts && bb summarize-log --format github "${{ inputs.benchType }}" "$log_path")"
          echo "## Query summary:" >> "$GITHUB_STEP_SUMMARY"
          echo "$summary" >> "$GITHUB_STEP_SUMMARY"

      - name: Compose Slack Message
        id: compose
        run: |
          type="${{ inputs.benchType }}"
          [ "$type" = "tpch" ] && type="TPC-H"
          [ "$type" = "auctionmark" ] && type="AuctionMark"
          [ "$type" = "readings" ] && type="Readings"
          [ "$type" = "yakbench" ] && type="Yakbench"

          log_path="$(pwd)/benchmark.log"
          summary="$(cd modules/bench/cloud/scripts && bb summarize-log --format slack "${{ inputs.benchType }}" "$log_path")"

          if [ -n "$summary" ]; then
            printf -v base '%s Benchmark completed — {status_message}\n\n%s' "$type" "$summary"
          elif [ -n "${{ steps.logs.outputs.time_taken_iso }}" ]; then
            base="$type Benchmark completed in ${{ steps.logs.outputs.time_taken_iso }} — {status_message}"
          else
            base="$type Benchmark cleanup — {status_message}"
          fi

          {
            echo "msg<<EOF"
            echo "$base"
            if [ "${{ steps.upload-logs.outcome }}" = "success" ] || [ "${{ steps.upload-grafana.outcome }}" = "success" ]; then
              echo ":link: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|view artifacts>"
            else
              echo ":link: (artifacts upload failed)"
            fi
            echo "EOF"
          } >> "$GITHUB_OUTPUT"


      - name: Post Slack Notification
        uses: ravsamhq/notify-slack-action@be814b201e233b2dc673608aa46e5447c8ab13f2 # v2
        if: steps.check-deployment.outcome == 'success'
        with:
          status: ${{ steps.derive.outputs.bench_status || inputs.status }}
          notification_title: "*${{ inputs.benchType == 'tpch' && 'TPC-H' || inputs.benchType == 'auctionmark' && 'AuctionMark' || inputs.benchType == 'readings' && 'Readings' || inputs.benchType == 'yakbench' && 'Yakbench' || inputs.benchType }} Benchmark*"
          message_format: ${{ steps.compose.outputs.msg }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.BENCHMARK_SLACK_WEBHOOK_URL }}
