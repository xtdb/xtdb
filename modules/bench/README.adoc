= Bench

== Running from Gradle

(from the root of the repo, values given are defaults)

TPC-H::
`./gradlew tpch -PscaleFactor=0.01`

AuctionMark::
`./gradlew auctionmark -Pduration=PT30S -Pthreads=8 -PscaleFactor=0.1`

Readings::
`./gradlew readings -PdeviceCount=10000 -PreadingCount=10000`

Products::
`./gradlew products`

IngestTsOverhead::
`./gradlew ingest -PdocCount=100000 -PbatchSizes=10,100,1000`


TSBS IoT::
`XTDB_LOGGING_LEVEL_BENCH_TSBS=debug ./gradlew :tsbs-iot -Pfile=/path/to/txs.transit.json`
+
For any reasonable scale you'll want to create a txs file beforehand - they take quite a while to generate in XT format.
See datasets -> `xtdb.tsbs`.

Add a YourKit snapshot with `-Pyourkit` - you should see a snapshot appear in your snapshot directory (default `~/Snapshots`) when the application shuts down.


In case you want to save the JSON benchmark log records to file you can append `-PbenchLogFile=tpch-0.01.log` to the command.

=== Running with custom config

You can run the benchmarks with a custom node config by passing `-PconfigFile path/to/config.yaml` to the gradle command.

`./gradlew auctionmark -PconfigFile=custom-config.yaml -Pduration=PT30S -Pthreads=8 -PscaleFactor=0.1`

== XTDB Bench Docker Image

The xtdb-bench Docker image is automatically built and pushed to GitHub Container Registry (GHCR) against the main branch every night. (tagged with `nightly`)


You can also manually build and push the image from any branch using the workflow_dispatch trigger in the "Build xtdb-bench Docker Image" GitHub Actions workflow (tagged with the Git SHA).

=== Running the Docker Image

From the root of the repository, you can run a benchmark with:

[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly <benchmark-type> <opts...>
----

Replace <benchmark-type> with the name of the benchmark, and <opts...> with any additional CLI arguments (e.g., --node-dir, --config-file, etc.).

TPC-H::
[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly tpch \
  --scale-factor 0.01
----
AuctionMark::
[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly auctionmark \
  --duration PT30S \
  --scale-factor 0.1 \
  --threads 8
----
Readings::
[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly readings \
  --devices 10000
  --readings 10000
----
Products::
[source,bash]
----
# ensure you've either downloaded the products dataset
# from S3 to datasets/products.transit.msgpack.gz,
# or your process has permissions to do so

docker run --rm ghcr.io/xtdb/xtdb-bench:nightly products
----

==== Running with custom config

You can run the benchmarks with a custom node config by passing `--config-file path/to/config.yaml` to the docker command.

[source,bash]
----
docker run --rm ghcr.io/xtdb/xtdb-bench:nightly auctionmark \
  --config-file custom-config.yaml \
  --duration PT30S \
  --threads 8 \
  --scale-factor 0.1
----

Included within the xtdb-bench are a number of config files for different cloud providers - see those under `cloud/config/`.

== Prometheus and Grafana

For local node monitoring, bring up a Prometheus and Grafana instance via `docker-compose up`.
By default the node serves Prometheus metrics under `localhost:8080/metrics` which Prometheus scrapes every 2 seconds.
If the metrics are not exposed under `localhost:8080` you need to tweak the `.config/prometheus.yaml` config accordingly.

Under `localhost:3001` you are able to access the Grafana UI (`admin` for user and password).
The XTDB node datasource should be setup, and there should be a dashboard available with some basic metrics.
If you tweak the dashboard or add a new one, save the changes in the corresponding file in `.config/dashboards/dashboard_name.json`.

== Running the benchmarks on the Cloud

=== Prerequisites

* AWS/Azure/GCP CLI
* Terraform
* Helm 3.0+

=== Authentication

Prior to running the benchmarks on the cloud, you need to set up your cloud provider credentials:

AWS::
[source,bash]
----
# Setup a profile named "xtdb-bench"
aws configure sso --profile xtdb-bench
# Logon to your profile:
aws sso login --profile xtdb-bench
----

Azure::
[source,bash]
----
# Login to Azure
az login --scope https://management.azure.com//.default
# Set the subscription
az account set --subscription "XTDB long-run reliability"
----

Google Cloud::
[source,bash]
----
# Login to GCP
gcloud auth login
# Set the project
gcloud config set project xtdb-scratch
----

=== Setup the Infra

We can setup the cloud infra using the "setup-infra.sh" script:

[source,bash]
----
./cloud/setup-infra.sh <aws|azure|google-cloud>
----

This will create the necessary resources for running the benchmarks on the cloud, and log you into the created Kubernetes cluster.

=== Run a benchmark

==== Basic Usage

Run benchmarks using the `run-bench.sh` script:

[source,bash]
----
./cloud/run-bench.sh <aws|azure|google-cloud> <benchmark-type> [helm-args...] [--no-cleanup]
----

==== Benchmark Types

TPC-H::
[source,bash]
----
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01
----

Readings::
[source,bash]
----
./cloud/run-bench.sh azure readings \
  --set readings.devices=10000 \
  --set readings.readings=10000
----

==== Common Configuration Options

===== Custom Docker Image

Override the default image tag to use `nightly` or a specific commit:

[source,bash]
----
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01 \
  --set image.tag=nightly

# Or use a specific commit SHA
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01 \
  --set image.tag=sha-abc1234
----

===== Skip Cleanup

By default, each run clears the cloud storage bucket and previous helm releases. To skip this cleanup:

[source,bash]
----
./cloud/run-bench.sh azure tpch \
  --set tpch.scaleFactor=0.01 \
  --no-cleanup
----

==== Auctionmark (Multi-Stage Benchmark)

Auctionmark typically runs in two stages: an initial load phase and an OLTP phase with multiple nodes/pods.

===== Load Phase

[source,bash]
----
./cloud/run-bench.sh azure auctionmark \
  --set auctionmark.scaleFactor=0.1 \
  --set auctionmark.onlyLoad=true
----

===== OLTP Phase

[source,bash]
----
./cloud/run-bench.sh azure auctionmark \
  --set auctionmark.threads=8 \
  --set auctionmark.duration=PT10M \
  --set auctionmark.noLoad=true \
  --no-cleanup
----

===== Monitoring OLTP Phase

To view logs from individual pods during the OLTP phase:

[source,bash]
----
# List all pods
kubectl get pods -n cloud-benchmark

# View logs from a specific pod (replace <pod-name> with actual pod name)
kubectl logs -f <pod-name> -n cloud-benchmark

# View logs from all pods with a specific label
kubectl logs -f -l app.kubernetes.io/name=xtdb-benchmark -n cloud-benchmark
----

=== Clear Benchmark Data Only

NOTE: Unless --no-cleanup is passed to the run-bench.sh script, this script will get called automatically at the beginning of each benchmark run.

If you need to clear benchmark data without destroying infrastructure, use the clear-bench script:

[source,bash]
----
./cloud/clear-bench.sh <aws|azure|google-cloud>
----

This will:

* Clear the Helm release
* Delete Kafka PVCs
* Empty the cloud storage bucket

=== Cleanup the Infra

To fully cleanup the storage, helm deployments and infrastructure on the cloud, you can run the "delete-infra.sh" script:

[source,bash]
----
./cloud/delete-infra.sh <aws|azure|google-cloud>
----

== Extending

If you need to add a new benchmark, you can do the following:

1. Create a new Clojure file in src/main/clojure/xtdb/bench/ for your benchmark implementation
2. You'll need to build and push the xtdb-bench docker image, you can do this via the "Build xtdb-bench Docker Image" GitHub Actions workflow if you've pushed your changes to the main branch. You'll also need to update the image tag in cloud/helm/values.yaml
3. Add a gradle task with createBench in build.gradle.kts
4. Update the cloud/run-bench.sh to include the new benchmark
5. Create a helm template for the new benchmark in cloud/helm/templates/
6. Update the cloud/helm/values.yaml with the new benchmark configuration

== CI and Grafana Dashboard Images

=== GitHub Actions

The repository includes CI workflows that can provision cloud infra, run the benchmark, and publish results.

At a high level:

* A nightly job will run benchmark workloads (e.g. TPC-H) against a permanent benchmark kubernetes cluster
* After the run, scripts export dashboard snapshots from Grafana for the time range of the run
* These snapshots (and other run artifacts) are uploaded to the workflow as GitHub Actions _artifacts_

=== How to download Grafana dashboard images from a job

1. Open GitHub → `Actions` → choose the latest benchmark run workflow
2. Scroll to the bottom to the **Artifacts** section
3. Download the artifact that contains Grafana snapshots (the name includes `grafana`)
4. Extract the archive locally to view the exported PNG images