benchType: tpch

auctionmark:
  onlyLoad: false
  noLoad: false
  scaleFactor: 0.1
  nodeCount: 3 # only applies if not onlyLoad
  threads: 8
  duration: "PT24H"

tpch:
  scaleFactor: 0.01

# Kubernetes Job settings
job:
  # Time-to-live for Jobs after they finish (in seconds)
  # Set to 0 to disable cleanup by TTL controller
  ttlSecondsAfterFinished: 3600

readings:
  devices: 10000
  readings: 10000

yakbench:
  scaleFactor: 0.1

xtdbConfig:
  # Core environment variables
  kafkaBootstrapServers: xtdb-benchmark-kafka.cloud-benchmark.svc.cluster.local:9092
  logTopic: xtdb-log
  jdkOptions: "-Xmx3000m -Xms3000m -XX:MaxDirectMemorySize=5000m -XX:MaxMetaspaceSize=500m --enable-native-access=ALL-UNNAMED"
  loggingLevel: info
  localDiskCache:
    sizeLimit: "150Gi"

providerConfig:
  # Set by cloud provider override
  env: {}
  # Additional labels to apply to benchmark pods.
  # Note: the chart reserves the following labels and will override any values provided here:
  #   - app.kubernetes.io/name
  #   - app.kubernetes.io/component
  # This is to keep Service selectors stable.
  podLabels: {}
  serviceAccountAnnotations: {}
  nodeSelector: {}
  nodeConfig:

image:
  repository: ghcr.io/xtdb/xtdb-bench
  pullPolicy: Always
  tag: "latest"

resources:
  limits:
    cpu: "2000m"
    memory: "10G"
    ephemeral-storage: 180Gi
  requests:
    cpu: "1750m"
    memory: "10G"
    ephemeral-storage: 180Gi

# For our bitnami/kafka deployment
kafka:
  image:
    registry: docker.io
    repository: bitnamilegacy/kafka
    tag: 4.0.0-debian-12-r10
    pullPolicy: Always
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
  controller:
    persistence:
      size: 50Gi
    resourcesPreset: medium
