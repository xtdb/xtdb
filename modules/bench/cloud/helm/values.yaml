benchType: tpch

auctionmark:
  onlyLoad: false
  noLoad: false
  scaleFactor: 0.1
  nodeCount: 3 # only applies if not onlyLoad
  threads: 8
  duration: "PT24H"

tpch:
  scaleFactor: 0.01

# Kubernetes Job settings
job:
  # Time-to-live for Jobs after they finish (in seconds)
  # Set to 0 to disable cleanup by TTL controller
  ttlSecondsAfterFinished: 3600

# Benchmark timeout (ISO-8601 duration, e.g. PT1H30M for 90 minutes)
# If set, the benchmark will gracefully abort after this duration
timeout: ""

readings:
  devices: 10000
  readings: 10000

yakbench:
  scaleFactor: 1.0

clickbench:
  size: small
  limit: ""
  # Name of k8s secret containing AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION
  awsSecretName: "xtdb-aws-datasets-secret"

fusion:
  devices: 10000
  readings: 1000
  duration: "PT30M"
  threads: 4

ingestTxOverhead:
  docCount: 100000
  batchSizes: "1000,100,10,1"

patch:
  docCount: 500000
  patchCount: 10

products:
  limit: ""
  awsSecretName: "xtdb-aws-datasets-secret"

ts-devices:
  size: small
  awsSecretName: "xtdb-aws-datasets-secret"

tsbs-iot:
  devices: 2000
  queryIterations: 20
  workers: 4
  burnIn: 5
  timestampStart: "2020-01-01T00:00:00Z"
  timestampEnd: "2020-01-07T00:00:00Z"

xtdbConfig:
  # Core environment variables
  kafkaBootstrapServers: xtdb-benchmark-kafka.cloud-benchmark.svc.cluster.local:9092
  logTopic: xtdb-log
  jdkOptions: "-Xmx3000m -Xms3000m -XX:MaxDirectMemorySize=5000m -XX:MaxMetaspaceSize=500m --enable-native-access=ALL-UNNAMED -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/dumps -XX:+ExitOnOutOfMemoryError"
  loggingLevel: info
  localDiskCache:
    sizeLimit: "150Gi"

dumpUpload:
  # Sidecar container resources for heap dump upload
  # azure-cli image + tdnf install needs ~512Mi during package install
  resources:
    requests:
      memory: "256Mi"
      cpu: "50m"
    limits:
      memory: "768Mi"
      cpu: "200m"
  # Size limit for the dumps volume
  volumeSizeLimit: "10Gi"

providerConfig:
  # Set by cloud provider override
  env: {}
  # Additional labels to apply to benchmark pods.
  # Note: the chart reserves the following labels and will override any values provided here:
  #   - app.kubernetes.io/name
  #   - app.kubernetes.io/component
  # This is to keep Service selectors stable.
  podLabels: {}
  serviceAccountAnnotations: {}
  nodeSelector: {}
  nodeConfig:

image:
  repository: ghcr.io/xtdb/xtdb-bench
  pullPolicy: Always
  tag: "latest"

# Git metadata for tracking benchmark versions (stored in annotations and env vars)
git:
  sha: ""
  branch: ""
  repo: "xtdb/xtdb"
  runId: ""

resources:
  limits:
    cpu: "2000m"
    memory: "10G"
    ephemeral-storage: 180Gi
  requests:
    cpu: "1750m"
    memory: "10G"
    ephemeral-storage: 180Gi

# Kafka deployment using official apache/kafka image
kafka:
  image:
    repository: apache/kafka
    tag: "4.0.1"
    pullPolicy: IfNotPresent
  persistence:
    size: 50Gi
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
