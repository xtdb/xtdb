= kafka-connect

== Schema-Driven XTDB Encoder Transformation

Allows for encoding the value of a record to insert each field using the appropriate XTDB type, based on the record schema.

Transforms into a value of type Map, and dismisses the value schema, as the value no longer complies with it.

=== Config

Prefix with `transforms.<name>.`

[cols="1,1"]
|===
|`type`
|`"xtdb.kafka.connect.SchemaDrivenXtdbEncoder"`
|===

=== XTDB types depending on schema

(Tentative)

Integer and float sizes are stored to the smallest size available in XTDB.

For strings, you can use a custom `xtdb.type` parameter for more specific XTDB types:

* `interval`
* `timestamptz`

== XTDB Sink Connector

== Config

=== `connector.class`

Must be `"xtdb.kafka.connect.XtdbSinkConnector"`

=== `jdbcUrl`

Required. Must point to XTDB's PostgreSQL-compatible port.

Example: `"jdbc:xtdb://my_host:5432/xtdb"`

[cols="1,1"]
|===
|Type:
|`String`

|Default:
|`None`

|Importance:
|`High`
|===

=== `id.mode`

Where to get the `_id` from.

One of:

record_key::
* The record key must be either a Struct or a primitive value
* If the key is a struct then `id.field` must be used to select a field to use as the `_id`
* *Required* if you want https://kafka.apache.org/documentation/#design_compactionbasics[tombstones] to delete records
record_value::
* `id.field` must be used to select a field to use as the `_id`

[cols="1,1"]
|===
|Type:
|`String`

|Default:
|`None`

|Importance:
|`High`
|===

=== `id.field`

The field name to use as the `_id`.
Leave blank if using a primitive `record_key`.

The behaviour depends on `id.mode`:

record_key::
* If the key is primitive then `id.field` *must* be empty string
* If the key is a Struct then must select a field from the Struct
record_value::
* Must select a key from the Struct

[cols="1,1"]
|===
|Type:
|`String`

|Default:
|`""`

|Importance:
|`Medium`
|===

=== `validFrom.field`

The field name to use as `_valid_from`.
Leave blank to use xtdb's default `_valid_from`.

[cols="1,1"]
|===
|Type:
|`String`

|Default:
|`""`

|Importance:
|`Low`
|===

=== `validTo.field`

The field name to use as `_valid_from`.
Leave blank to use xtdb's default `_valid_from`.

[cols="1,1"]
|===
|Type:
|`String`

|Default:
|`""`

|Importance:
|`Low`
|===

=== `table.name.format`

A format string for the destination table name, which may contain `${topic}` as a placeholder for the originating topic name.

[cols="1,1"]
|===
|Type:
|`String`

|Default:
|`"${topic}"`

|Importance:
|`Medium`
|===

== Example config

[source,json]
----
{
    "name": "xtdb-jdbc-sink-connector",
    "config": {
        "connector.class": "xtdb.kafka.connect.XtdbSinkConnector",
        "tasks.max": 1,
        "topics": "readings",
        "url": "http://xtdb:3000",
        "id.mode": "record_value",
        "id.field": "xt/id",
        "validFrom.field": "validFrom",
        "validTo.field": "validTo",
        "table.name.format": "companyco_${topic}",
        "transforms": "convertValidFrom,convertValidTo",
        "transforms.convertValidFrom.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
        "transforms.convertValidFrom.field": "validFrom",
        "transforms.convertValidFrom.target.type": "Timestamp",
        "transforms.convertValidFrom.format": "yyyy-MM-dd'T'HH:mm:ssX",
        "transforms.convertValidTo.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
        "transforms.convertValidTo.field": "validTo",
        "transforms.convertValidTo.target.type": "Timestamp",
        "transforms.convertValidTo.format": "yyyy-MM-dd'T'HH:mm:ssX",
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "key.converter.schemas.enable": "true",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter.schemas.enable": "true"
    }
}
----

With some data that looks like:
[source,json]
----
{
    "schema": {
        "type": "struct",
        "fields": [
            { "type": "int64", "optional": false, "field": "xt/id" },
            { "type": "string", "optional": false, "field": "metric" },
            { "type": "int32", "optional": false, "field": "measurement" },
            { "type": "string", "optional": false, "field": "validFrom" },
            { "type": "string", "optional": false, "field": "validTo" }
        ]
    },
    "payload": {
        "xt/id": 1,
        "metric": "Humidity",
        "validFrom": "2024-01-01T00:15:00Z",
        "validTo": "2024-01-01T00:20:00Z",
        "measurement": 0.8
    }
}
----

If storing datetimes with timezones is important to you, I would suggest
writing a https://docs.confluent.io/platform/current/connect/transforms/custom.html[custom transform].

== Development

REPL:

[source,bash]
----
$ clj -M:dev
----

Build:

[source,bash]
----
$ clj -T:build uber
----
