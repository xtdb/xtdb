* Core2
** <2021-01-19 Tue>

*** Getting used to Arrow

tx-ingest index
- row index

#+begin_src clojure
{:row-ids [0 1 2 3]
 :blobs [{:battery-level 0.84
          :ssid :s1}
         {:battery-level 0.12
          :ssid :s2}
         ...]}
#+end_src

per tx
- column of pairs: row-id, nippy blob
- column of pairs: row-id, arrow IPC message

=VariableLengthList<Union<...>>=

content index

if this were 'normal, schema-aot relations'
schema:
'each record block has row-ids (long), ssid (string), battery-level (double)'

record batch:
metadata (count of records) + extension (minmax, bloom filter, unique count, histogram, etc, per column)
could store metadata inline (using Arrow's built-in metadata support) or separately (different file)
one length-3 record batch would be =0 2 5, :s1 :s2 :s4, 0.84 0.12 0.56=

metadata for entire file - end of file or own file

block - range of bytes
'schema block' - how to interpret record batch blocks
'record batch blocks' - columnar data
  not self-describing without schema block
'dictionary block' - mapping from ids to strings
'dictionary delta block' - changes from the previous dictionary

ingest for a batch of transactions

**** content-idx options

***** option A - all columns in one file
#+begin_src clojure
  {:row-ids [0 1 2 5]
   :crux.db/id [.. ..]
   :ssids [:s1 nil :s2 :s4]
   :battery-level [0.84 nil 0.12 0.56]
   :user-name [nil "James" nil nil]}

  ;; + 'real' Arrow file
  ;; + row-id stored once (not massive, but still a benefit)
  ;; - loads of nils for dynamic data?
#+end_src

***** option B - file per column

#+begin_src clojure
;; chunk 1, file 0
{:row-ids [0 2 5]
 :ssids [:s1 :s2 :s4]}

;; chunk 1, file 1
{:row-ids [0 2 5]
 :battery-level [0.84 0.12 0.56]}

;; chunk 1, file 2
{:row-ids [1]
 :user-name ["James"]}
#+end_src

projection is hard here
can we align row-ids between files in a chunk? yes, possibly, using the validity bitmap
can we align blocks between the files in a chunk?

***** option B2 - alignment

#+begin_src clojure
;; chunk 1, file 0
{:ssids [:s1 nil :s2 nil nil :s4]}

;; chunk 1, file 1
{:battery-level [0.84 nil 0.12 nil nil 0.56]}

;; chunk 1, file 2
{:user-name [nil "James" nil nil nil]} ; or truncated
#+end_src

***** option C - hybrid - file per schema
tx-ingest index
- a: don't have it
- b: use it for projections

#+begin_src clojure

  ;; row per operation
  [{:op :put
    :row-id 41
    :doc {} ; union of structs? byte array? arrow extension type - variable length byte array?
    :start-valid-time #inst "2021"
    :tx-id 15
    :tx-time #inst "2021"
    }

   {:op :delete
    :row-id 41}]

  ;; hybrid - no schema re-use
  [{:tx-time ...
    :tx-id ...
    :ops [{:op :put
           :row-id 41
           :schema-id 12
           :doc 'ipc-message}
          {:op :put
           :row-id 42
           :schema-id 12
           :doc 'ipc-message}]}]

  ;; hybrid - with schema re-use
  [{:tx-time ...
    :tx-id ...
    :ops [{:op :put
           :row-id 41
           :schema-id 12
           :doc 'record-batch-block}
          {:op :put
           :row-id 42
           :schema-id 12
           :doc 'record-batch-block}]
    :union-schema 'schema}]

  ;; row per transaction
  [{:tx-time ...
    :tx-id ...
    :ops 'ipc-message}]

  ;; should this contain transaction metadata too?
  ;; block per transaction? - WAL

  ;; what's the format on Kafka? Arrow? IPC?
  ;; would need less of a serde before ingest
  ;; parallelism - removes work from the critical serialised section
  ;; messages:
  ;; - schema: union of all docs in the transaction
  ;; - 1 record-batch
  ;; - row per operation

  ;; no longer infinite retention
  ;; no longer evicting from the tx-log

  ;; could steal some ideas from Flight
  ;; Kafka - schema registry - no longer self-describing though

  ;; alternative: row per transaction

  ;;;; content index

  [{:row-ids [0 2 5]
    :crux.db/id [... ... ..]
    :ssids [:s1 nil :s4]
    :battery-level [0.84 0.12 0.56]}

   {:row-ids [1]
    :user-name ["James"]}]

  [{:live-file 0
    :cols #{:crux.db/id :ssids :battery-level}}
   {:live-file 1
    :cols #{:crux.db/id :user-name}}
   {:live-file 2
    :cols #{:crux.db/id :ssids}}]
#+end_src

in option C, how do we make the cutoff?
same problem as option B, I think?
maybe when one of the live files goes over an AV limit
(using AVs as the limit as this is the metric that file size is most proportional to?)
could use transactions, or rows, but AVs is more granular - maybe bytes?
could use something more complex that takes all the live files into account - so long as it's deterministic, it doesn't really matter

might have to pull down many chunks to find `:name "James"` - particularly `:crux.db/id` (used within joins)

need explicit row ids because documents have dynamic structure
should we optimise for cold nodes answering low-latency queries? hard...

**** questions

cardinality many? vectors/sets
1. ignore
2. solve, but just vectors (variable length lists) - minmax/bloom would flatten
2b. add an extension type for sets
3. duplicated row-ids (works for B)

entity id as a column?
works well in options A/C - entity ID is right next door to the data

maps as values - nippy? nested struct?

metadata 'latest-completed-tx'
metadata 'latest row-id'

**** unions
dense:: Pair<List<Double>, List<Long>> + Map<Idx, Type & Offset>
^^ we probably want this one

sparse:: List<Pair<Double, Long>>
#+begin_src clojure
[[0.0 nil]
 [nil 4]
 [12.0 nil]]
#+end_src

*** access patterns
AVE AV AEV AE, project-*

#+begin_src clojure
  '{:find [?a],
    :where [[?e :name "Ivan"],
            [?e :age ?a]]}
#+end_src

find =:name= "Ivan", get a bunch of row-ids, look up =:age= for those row-ids

#+begin_src clojure
  '{:find [?n], :where [[_ :name ?n]]}
#+end_src
scan =:name= cols
currently: scan AE -> filter from bitemp
B is preferable here - general 'projecting a small subset' advantage
parameter: visibility ratio
bitemp index split by attribute (/set of attributes) -> scan for row-ids,

project-* works quite nicely in C and A - the whole document is in the same record batch
downside is that projecting a single field requires transferring the whole doc.

'pull-like', navigational join
#+begin_src clojure
  '{:find [?order #_?line-item ?product-name]
    :where [[?order :order-id "1234"]
            [?line-item :order ?order]
            [?line-item :product ?product]
            [?product :product-name ?product-name]]}

  '{:find [(pull ?order [{:line-item {:product [:product-name]}}])]
    :where [[?order :order-id "1234"]]}
#+end_src

'proper' join
#+begin_src clojure
'{:find [?student]
  :where [[?enrollment1 :student ?student]
          [?enrollment1 :course :maths]
          [?enrollment2 :student ?student]
          [?enrollment2 :course :science]]}
#+end_src clojure

assume science is smaller

intersection
nested-loop join - if both sides are really small
hash join - default to this
  hash of student to enrollment for the smaller, scan larger
  can spill hash buckets to disk if need be
sort/merge join

*** temporal use cases
temporal parameters
number of visible entities?
how many updates to any one entity?

assumption: entity/vt pair, 0-5 tt updates
would you _model_ any cases this way?

tweets
ever-growing, ~all visible now, edit/delete in vt=now 0-5 times, no tt updates
possible evict
delete cascade

customer details
relatively constant cardinality, ~all visible now, tens of vt updates/deletes, 0-5 tt updates for a vt

orders
how to model 'status'? either delete when complete, or set status flag (JH preferred)
ever growing cardinality, ~all visible now, tens of vt updates/deletes, 0-5 tt updates for a vt

active sessions - transient
ever growing cardinality, few visible now, delete when you're done

share prices - time series
model as vt being the current share price
relatively constant cardinality, ~all visible now, small visible set, loads of vt updates/deletes, 0-5 tt updates for a vt
or, model as events -> ever growing cardinality, few vt updates, few tt updates

common denominator - entity identity
order-id is the identity - valid time represents that order changing over time
share ticker is the identity - likewise

*** implementation ideas
no natural clustering by ticker
cluster by row-id
batch would then have 16 files - tradeoff is searching by anything you _haven't_ clustered by is painful

what if we could assume that vt=tt for the 'majority of updates'?
bitemp index aims: immutable, append-only, columnar

roughly sorting by VT is still append-only (except when it's not)
sorting by TT _is_ append-only - but we don't often want to sort first by TT
sort by VT within a bitemp index chunk

in bitemp index, need to find based on row-id
or remove row-id? VT as a column

let's say we have hundreds of bitemp index chunks
- 'is this row-id valid at the time of this query?' (timeslice)
- 'what is the current row-id for this entity-id?' - include the (hash of the) entity-id in the bitemp index, as a column?
- 'when was this row-id valid?' (temporal ranges)

attribute (sets) slicing would be worth exploring

[?e :name "Ivan"]
#{:name :age} -> bitemp index slice

modelling VT corrections like evictions?

ok. let's say bitemp index had entity-id (hash), row-id (+ve/-ve), vt, tt cols
overall file would have metadata
- attribute sets contained
- entity id, abs(row-id) bloom filters (on record batches too)
- vt histogram/minmax, tt minmax (likewise)

ingest would need to make queries to generate list of row-ids affected by tx-op (same as currently)

questions from above:
- 'is this row-id valid at the time of this query?' (timeslice)
  - chances are we'd have attribute set in hand (because we've come from the content idx)
  - only need to check bitemp index files with vt/tt less than query time
  - could say 'definitely no/maybe yes' from bloom filters
- 'what is the current row-id for this entity-id?'
  - entity-id bloom filter, starting at file with latest vts/tts
- 'when was this row-id valid?' (temporal ranges)
  - just check row-id bloom filters, ignore above temporal filters

*** Temporal musing (jms) <2021-03-08 Mon>

what things do append-only make difficult?

one concern is around looking up the history of IBM's ticker.
- let's say you model it using ticker name, you're then going to get a lot of rows with "IBM" as :ticker/name
- you could model it as the entity ID, but that's only useful for one such attribute
- split it out per-attribute, and slowly changing data can still go under the same entity id - but you have to resolve per-attribute
- this would also make a row 'partially valid' - some of the attributes will remain the same, others will be out of date.

you can't go back and edit the end time of a row, so you can't then say when this row is valid until.
maybe I'm not interested in that - maybe I'm interested in whether this one column is still valid.

at 2021-01-01, select price where ticker/name = "IBM"

presumably you quickly get to a point where you have the possibly matching entity-ids at hand.
it's not distinct, but it is deduped

let's say we don't include a value in metadata (specifically, the Bloom filter) if it's not new
that would mean that searching for =:ticker/name "IBM"= would give us the first instance of that entity, get us _one_ entity ID to resolve.
it may be that later entity-ids for the IBM ticker also have =:ticker/name "IBM"=, but if we're doing a timeslice query, we'll only have one row to re-check.
bitemp? we could only look back at the values we're replacing, potentially.
this would introduce deduping - we wouldn't want duplicate IBMs in the result
  but presumably this is an issue already? why might it not be an issue already?
    atm we find all rows with IBM, filter by what's valid
  two bloom filters?

problem: when we have n versions of an entity, and we search by an unchanging attribute, we want closer to 1 lookup than n
idea: bringing the attribute values into the temporal index?
will be an issue for wide docs
more general idea here is that however we store the temporal index also gives us an idea (without further lookups) of the delta of a document

the dual of this is that, if we were to only store deltas, this index would give us an idea of the where the rest of the row was
- which other row-ids (chunks? blocks?) have values that are still live in this row?
  - would this actually make selected queries faster?
  - how does this work in bitemp when you're not necessarily only replacing one row, and maybe only for part of its range?
  - what would this look like?

looking at the use cases, and their queries:
- tweets
  - tweet by id
    - only one tweet with that id.
    - don't want to make too many (any?) requests to confirm it's the current version
  - timeline for a user
    - find tweets for a user, order by time desc
    - scan for =:tweet/user-id "jarohen"=,
    - again, for each tweet, don't want to make too many requests to confirm it's the current version
- customer details
  - customer by id, current
    - same as tweet-by-id
  - customer by id, as-of
    - same, but with a watermark
  - customer by unique key (email, say)
    - similar to timeline-for-a-user
- orders
  - order by id, current state
  - order by id, history
- active sessions
- share prices (modelled by valid-time, eid = ticker)
  - current price
  - price as-of vt
  - price history

thoughts while writing out the above:
- real handy if we didn't need to scan for temporal data - e.g. if it were block-aligned with the data itself.
- TPC-BiH uses user-defined times as valid-times - do we want to do the same?
- sibling temporal indices - like eviction, we could add revisions for the temporal index of a chunk

'just enough schema'
- would a =:crux/type= key really help us out here, as a means of partitioning data, even if the schema of a type isn't fixed, and an entity isn't restricted to one type over time?
- maybe partitioning by the attribute namespace?

** <2021-01-25 Mon>
*** Metadata
Problem: how do we efficiently intersect content + bitemporal?
Rocks had compaction - there'd only be one entry for one AVE - we have them spread over many chunks
We don't have an easy way of knowing whether the entry in one chunk is the same entity as we've already seen, albeit with a different row id
How do we know what row-ids are visible?

idea: reducing resolution/low fidelity of the timeline index - hopefully rule out many row ids early

idea: metadata - current vs historical?
- would mean revisions to metadata
- revisions would be heavy, they'd need to scan the whole block.

assumption: 'this would be easy in Postgres' - why?
- sorted local indexes
- co-located content + bitemporal - start/end cols, update in place

cpuavg-1m, 5m, 15m, add row 6
valid-time V1, T1 - valid from, tt from
missing valid-to, tt-to
replaces row 4
one row can be replaced more than once, at different VTs
- say put R1 01-10, put R2 06-07, put R3 05-08
- in this example, R1 then has two validity periods
  - how does timeline index deal with this?
  - could always split and create a new row-id, I guess
    - this would then mean a row can only ever have one validity period?
      - nope - R1 in the above example would still have vt-end 10 up to R2, 06 up to R3, and 05 thereafter
      - in a 2D graph, its validity would be quite the polygon
      - we could start looking from the latest TT with -1 though

to invalidate this using metadata alone:
- -6 is in this chunk - can we do this with bloom filters?
- I have minmax valid-time/tt for this chunk

'tri-temporal'? - temporal table in an append-only fashion
need to supersede old row
a change results in more than one row - closes old row
does this make things easier in the vt=tt area?
two indices/bloom filters - one for vt=tt, one for vt!=tt

assumption: we're going to need to look through _every_ block that contains a given AV to find RIDs/Es, if that's what we're searching for (i.e. =[?e :name "James"]=)
if not, we'd need to know that, for a given AV, the only Es are ones we've already seen, which would involve putting AVE in the metadata, in some form?
k-d tree might be a good solution here to say 'definitely not'
  even then, though, we could only ever say 'probably', unless the tree were known to be complete.
maybe we could, 'every now and again', have a complete AVE for certain values, as a checkpoint - would need to be adaptive, though
to get around this assumption, we'd be looking for a magical bloom-like filter where you could query for 'this V but not this E'

append only KD-tree - binary tree, but change sorting per level https://en.wikipedia.org/wiki/Implicit_k-d_tree
K2 trees - succinct quad tree - https://www.researchgate.net/publication/221580271_k2-Trees_for_Compact_Web_Graph_Representation
'Advanced Indexing for Temporal Data' - https://www.researchgate.net/publication/50370124_Advanced_Indexing_Technique_for_Temporal_Data/fulltext/0e60c876f0c493afa4b73b5e/Advanced-Indexing-Technique-for-Temporal-Data.pdf

idea: tiered metadata
this is arguably the same as adaptive indexing, in that we may not choose to make all the indexes for all the data AoT

apache data sketches? https://datasketches.apache.org/

** Object stores <2021-01-29 Fri>

Categories of OS: Memory, Local FS, Memcached, S3

Scenarios:
- start node
  - fetch all metadata chunks (probably via 'list')
- ingest
  - upload me a chunk (many files + metadata)
    - parallel upload of all chunks (except metadata?)
    - then, when they're all successful, upload metadata as 'done'/'committed' marker
- temporal indexing
  - to update the temporal index (simple one-dimensional put-only timeline)
    - find current/superseded row-id for this entity-id
    -
- query
  - all time
    - doesn't require temporal indexing - just scan content indices
      - read metadata, find chunks/blocks that match
  - at tt=TT (maybe tt=now)
    - find users where name = "James"
      - skip to TT, backward scan for name="James" (using metadata), forward scan eid (using metadata) to see if it's been replaced before TT
      - skip to TT, backward scan all names, track (name, eid, tx-id)

What is 'committed'?
- (crux/db node {:tx-time "12:07"})
  - any queries I make of that DB need to be replayable
    - no transactions going to be inserted _before_ {:tx-time "12:07"}
      - node's indexed to 12:04
      - as-yet unindexed transaction at 12:06, so I can't accept this yet
      - I need to have seen a transaction at 12:07 or later
- 'seen a transaction'
  - we'll increase an offset - all transactions below the offset are readable
  - if I do make a query that requires temporal data - wait for temporal indexing?
- 'written to object store' might be much later than 'committed'
- 'can reliably reproduce this state from the log' -> committed
  - I need to know that no more transactions can now be inserted _before_ this watermark

** Low-level algebra
*** <2021-02-23 Tue>
Operators: select, project, join, group/aggregate, order, top-n (slice)

what's the most helpful data structures they can be given, and what's the easiest thing for them to produce?

alignment is a cross-cutting concern:
- is it only select that needs to deal with this?
- other operators, if given aligned vectors, will return aligned vectors
- do we /force/ project to align its output?

'direct alignment': two vectors are aligned iff the same row-id is at the same index
implies same length

'indirect alignment': could be solved by mapping

'alignable': can find the right elements for a row-id

four levels:
- user-facing query engine(s) - planner that generates a relalg tree from a user query
- high-level, relalg tree - knows about relalg operators. logical plan
- intermediate-level - add opA, physical plan
- lower-level MIL-like column primitives

**** example select:
select a, b, c from foo where foo.a < 20 && foo.b < 10
- select a < 20 - output col is trivially aligned
- select b < 10
- intersecting selection vectors
- a valid output is unchanged a and b, plus intersected selection vector of row-ids

op 1: a -> select foo.a < 20
op 2: b -> select foo.b < 10
  - optionally input selection vector from a

**** operators:

scan:

operation A:
- for a vector of row-ids + cols, get me an aligned VSR
- exclude any rows that don't have all the cols present
- is this included in 'select'?

communication between operations:
- VSR + (possibly nil) selection vector of row-ids
- aligned VSR (including row-id cols) + (possibly nil) selection vector of indices <--

select:
- 'a < 20' - does depend on aligned a - alignment of a single is trivial
- 'a < 20 && b < 10' - do them one by one, 'and' the row-ids, /then/ execute opA to align them
- 'a < b' - does depend on at-least-indirect alignment
  - could be rejected as part of opA - essentially inlining the select
  - is there benefit to combining these two operators?
- first happens in column space, second happens in row space
- 'a + b < 20' is projection followed by selection
- /something/ needs to do vector alignment
- output options:
  - two dimensions here - aligned/not-aligned x selection vector applied
  - aligned VSR with selection vector applied
    - '/everything/ is aligned' -> simple
  - unchanged input VSR + selection vector of row-ids (vector contains filtered-out values)
    - avoid copies
    - aligned?
      - could return selection vector of indices
  - just outputs the selection vector
- select could also know a little about projection - given a set of columns to output

project:
- n cols -> n + 1 cols
- operation: "a -> a, c -> a + b"
  - input is a VSR with a and b vectors
- doesn't strictly need alignment, just that input is alignable
- is projection a filter in our world?

join:
- pipeline-breaking operator
  - 'free' to copy things, because we'll need to anyway
  - can inline until this point, at this point we have to do some work
    - 'terminal' vs 'intermediate' in Stream parlance
  - can save direct alignment until this point

group/aggregate:
- not required, but would benefit from being aligned

order:
- doesn't change shape
- requires input vectors to be directly aligned

top-n (slice):
- doesn't change shape of input data
- requires input vectors to be directly aligned
- essentially return a slice

*** <2021-02-24 Wed>

scan, deals with metadata, returns a (spl-)iterator of blocks - I/O operator
scan pure operator takes metadata root and decides whether it's worthy

question: how does iterator + async work?
JS - looks like a generator of promises, can use something similar here
tryAdvance takes a consumer, which would work with async by calling the consumer asynchronously, but that's probably not what you want?
traditional hasNext/next - hasNext would need to be async, next then wouldn't (assuming a buffer)
tryAdvance taking a consumer works better for resource management, if we wanted to re-use the VSR for the next block, say
downstream, will anything be async?

hasNext/next
hasNext :: CompletableFuture<Boolean>
next :: VectorSchemaRoot

real tryAdvance :: Consumer<A> -> Boolean

async tryAdvance :: Consumer<VectorSchemaRoot>, Executor -> CompletableFuture<Boolean>
async tryAdvance :: Consumer<VectorSchemaRoot?>, Executor -> void
async tryAdvance :: Consumer<CompletableFuture<VSR>> -> boolean

streams of VSRs?

flow/reactive-streams

then select

- case 1: select based on a single column
- case 2: select based on multiple columns sharing a row-ids
- case 3: select based on multiple columns with different row-ids

** Post operators plan <2021-03-04 Thu>
Next aim:
- Updates/deletes + transaction-time timeslice queries

* Profiling TPC-H SF0.1 Queries <2021-04-27 Tue>
times are YourKit times, see [[https://s3.console.aws.amazon.com/s3/buckets/test-jms?region=eu-west-1&prefix=core2-profiler-snapshots/&showversions=false][S3 bucket]]. queries were warmed up beforehand

** Q1, scan project group. overall 23s.
- scan align-roots 15s, 66% overall
  - align-vectors 8.5s, 37% overall, 56% of align-roots
    - du-copy 6.8s, 80% of align-vectors
      - write-type-id 4.5s, 66% of du-copy
        - set-value-count 2s, 8M calls, 30% of du-copy
      - DUV.reAlloc 0.7s, 10% of du-copy
  - createTemporalRoots 6s, 26% overall, 40% of align-roots
- aggregate-groups 3s, 15%
  - pointer-or-object 1.6s, 1.1M calls, 47% of aggregate-groups

** Q5, scan, join, group. overall 86s.
seems dominated by joins
- join, probe-phase 66s, 76% of overall
  - copy-tuple 61s, 4M calls, 93% of probe-phase, 71% of overall
    - du-copy (invokePrim) 53s, 34M calls, 80% of probe-phase
      - du-copy (invokeStatic) 48s, 34M calls, 72% of probe-phase
      - unaccounted 5s, 8% of probe phase, between invokePrim and invokeStatic
- scan, align-roots 14s, 16% of overall
  - createTemporalRoots 7.8s, 59% of align-roots, 10% of overall
  - align-vectors 5s, 37% of align-roots, 6% of overall
    - du-copy (direct to invokeStatic) 4s, 80% of align-vectors.

looking at du-copy (invokeStatic) - 52s, 36M calls, 60% overall:
- write-type-id 37s, 71% of du-copy
  - util/set-value-count, 19s, 73M calls, 37% of du-copy
  - another 6s dropped between write-type-id invokePrim and invokeStatic

** Q9, scans, PK joins, project, group, overall 33s.
seems dominated by scan:
- scan align-roots 20s, 61% overall
  - createTemporalRoots 11s, 34% overall
  - align-vectors 8s, 25% overall
    - du-copy 6s, 72% of align-vectors
- probe-phase (join) 8.5s, 25%
  - copy-tuple 6.7s, 80% of probe-phase
    - du-copy 5.8s, 68% of probe-phase

looking at du-copy:
- 12s, 7M calls, 35% overall
  - write-type-id 8.5s, 72% of du-copy
    - set-value-count 4s, 15M calls, 33% of du-copy
  - DUV.reAlloc 1.3s, 11% of du-copy

** DenseUnionUtil
trying to shave off some time by extracting du-copy out to a Java class, where we can be sure method calls aren't going via boxed intermediate methods.
seems a ~5-10% improvement on Q5 and Q9
*** pre
#'core2.tpch/tpch-q1-pricing-summary-report
"Elapsed time: 1133.470663 msecs"
#'core2.tpch/tpch-q5-local-supplier-volume
"Elapsed time: 3324.900396 msecs"
#'core2.tpch/tpch-q9-product-type-profit-measure
"Elapsed time: 2201.697073 msecs"

*** post
#'core2.tpch/tpch-q1-pricing-summary-report
"Elapsed time: 1178.077209 msecs"
#'core2.tpch/tpch-q5-local-supplier-volume
"Elapsed time: 3073.089175 msecs"
#'core2.tpch/tpch-q9-product-type-profit-measure
"Elapsed time: 2072.93805 msecs"

** reusing VSRs in operators

** probe-phase vectorisation
`probe-phase` was being dominated by calls to `copy-tuple`, which had a lot of per-row overhead (e.g. looking up vectors in VSR). we refactored probe-phase so that this overhead could be amortised over a block - it now keeps track of the probe-idxs to copy and copies them all in one go at the end of the block (`copy-tuples`). We still have to call `copy-tuple` individually for the build side because rows there can come from any build root.

*** post
#'core2.tpch/tpch-q1-pricing-summary-report
"Elapsed time: 1063.493334 msecs"
#'core2.tpch/tpch-q5-local-supplier-volume
"Elapsed time: 2409.118736 msecs"
#'core2.tpch/tpch-q9-product-type-profit-measure
"Elapsed time: 1749.365797 msecs"

** lazy mat
*** pre

#'core2.tpch/tpch-q1-pricing-summary-report
"Elapsed time: 3763.514114 msecs"
#'core2.tpch/tpch-q5-local-supplier-volume
"Elapsed time: 5349.604988 msecs"
#'core2.tpch/tpch-q9-product-type-profit-measure
"Elapsed time: 5286.252525 msecs"

regression?

*** post

#'core2.tpch/tpch-q1-pricing-summary-report
"Elapsed time: 3469.062123 msecs"
#'core2.tpch/tpch-q5-local-supplier-volume
"Elapsed time: 4836.539507 msecs"
#'core2.tpch/tpch-q9-product-type-profit-measure
"Elapsed time: 4885.72125 msecs"
