= CURRENT_TIMESTAMP as DB State

This document is written in response to link:bases.adoc[bases.adoc] and to make some of my thinking explicit as we reach the next phase, during we will start looking at resolving this in earnest.

Since the inception of Core2, my idea and plan has been that LogAppendTime should be the only canonical source of time. The main argument is that it should be easy to reason about what's happening, and to never introduce any implicit sources of time anywhere in the system. Unless I'm mistaken the key difference is that the Bases proposal implicitly uses the local node's clock to advance observable APPLICATION_TIME.

## Problem and Constraints

Must haves:

1. a single source of time derived from LogAppendTime -> SYSTEM_TIME / CURRENT_TIMESTAMP -> APPLICATION_TIME (unless provided as constraints).
2. monotonically increasing clocks within a session, across reads and writes.
3. read your own writes.
4. no implicit dependencies on the local nodes state, including the clock.

Should haves:

1. ability to perform repeatable reads.
2. UTC "original" timezone.
3. ability to override LogAppendTime during initial import.
4. no direct communication between the nodes.

Nice to haves:

1. CURRENT_TIMESTAMP advances in a intuitive way without introducing another clock than LogAppendTime.

## Proposal

1. CURRENT_TIMESTAMP, CURRENT_TIME and CURRENT_DATE are derived from LogAppendTime, called CURRENT_SYSTEM_TIMESTAMP here.
2. The CURRENT_SYSTEM_TIMESTAMP session variable is necessary to set for repeatable reads, as this provides an upper hard limit on what can be seen. Repeatable reads are not expected to be a massive use-case, but should be possible. This sits below the FOR SYSTEM_TIME AS OF family of predicates in SQL:2011 itself. (Drivers will usually guarantee read-your-own-writes within the current session implicitly by waiting for their submitted transactions to be processed, which doesn't require repeatable reads as such, just that the log has advanced enough.)
3. The CURRENT_SYSTEM_TIMESTAMP is really the tx-id in temporal and human readable form, and unique per transaction. (This isn't currently true when using Kafka which has millisecond precision, nor enforced in general.) See discussion below.
4. An option is to have CURRENT_MAX_VISIBLE_TX_ID instead, and derive CURRENT_TIMESTAMP from this to avoid confusion. This requires less magic, but introduces an opaque transaction id to the end user, which might be a good thing. See discussion below.
5. CURRENT_TIMESTAMP can be seen as a value stored in the database. It doesn't advance on its own: the database isn't a clock (in the wall-clock sense used to trivially tell the current time, it is very much a clock in the sense that it is the time oracle of XT). This is initially not intuitive, but a consequence of building an immutable, deterministic database. Queries can only see the state of the database and the parameters provided, nothing else.
6. APPLICATION_TIME can be provided explicitly by the user, for both as read constraints (AS OF, OVERLAPS etc.) and DML (FOR PORTION OF, APPLICATION_TIME_START/END).
7. During basic immutable usage, with some audit and corrections, where the data mainly sits in the past, the above won't really be noticed.
8. When one wants future APPLICATION_TIME data to appear as wall clock time advances, SQL:2011 in DEFAULT_APPLICATION_TIME ISO_MODE already sees this, as it doesn't filter anything. A hint maybe about why this is the default in SQL:2011?
9. When DEFAULT_APPLICATION_TIME AS OF CURRENT_SYSTEM_TIMESTAMP is set, there will be a dependency on a flow of transactions through the system for observable time to advance. When making queries into this potential future, the user can (and probably should) explicitly specify APPLICATION_TIME AS OF ? and provide it as a parameter, originating from an upstream event, or a time oracle like the client's local clock. We don't make any assumptions about this timestamp or when it will advance.
10. DEFAULT_APPLICATION_TIME could also support AS OF ?, which is a terser way of the above, impacting all tables.
11. DEFAULT_APPLICATION_TIME AS OF LOCAL capturing the node's local clock implicitly, seems intuitive, as it makes time advance. The main problem with this the inconsistency with write transactions in the same session. Writes this world always use DEFAULT_APPLICATION_TIME CURRENT_SYSTEM_TIMESTAMP as there's no need to guess. It also creates a coupling between the APPLICATION_TIME one wishes to query at and the LogAppendTime - which is what the local node's clock tries to emulate, not APPLICATION_TIME - which may or may not exist. If one wants this, one can do it explicitly via a parameter using any of the above methods. Several nodes implicitly and independently advancing APPLICATION_TIME feels like a potential source of confusion and unintentional inconsistency during write transactions, even if it is strictly deterministic. NTP should be used for basic sanity, but not attempted to be relied on.
12. In some cases, submitting a transaction to capture a new and current LogAppendTime to use for the processing may be the right thing to do, and derive some data from this which is stored and potentially later further processed. This might happen via triggers.

== LogAppendTime Resolution

We use micros internally for CURRENT_SYSTEM_TIMESTAMP, and Kafka uses millis for LogAppendTime. As discussed above in points 3 and 4, we would prefer to have a single, human readable timestamp unique per transaction. This is currently not the case, and especially when using millis, not possible if we take the LogAppendTime verbatim. I propose the following algorithm:

1. Convert LogAppendTime into micros.
2. If LogAppendTime is less than equal to CURRENT_SYSTEM_TIMESTAMP, set it to CURRENT_SYSTEM_TIMESTAMP + 1 micro.
3. Update and use CURRENT_SYSTEM_TIMESTAMP in the transaction.

This means there will be a slight drift when there are clashes between transactions sharing the same LogAppendTime. These clashes will happen when using millis, and are unlikely, but not impossible when using micros. As the original LogAppendTime is a monotonically increasing clock, taken from an NTP source, it will stay close to wall-clock time, but there's no inherent need for its exact precision here, as it's always a bit arbitrary to start with, as long as it advances. Actual drift could be a problem if there are sustained write throughput which results in many microsecond clashes in a row. We will sanity check this, but my feeling is that this is highly unlikely to be a real problem, due to the inherent latency and time it takes to append things to the log in the first place. It would also be a nice problem to have, as it would imply that the log would be able to sustain closer to 1M tps - the real value for this to become a problem is likely much lower, one could calculate the probability, but it's probably easier and safer to simply measure it. If we run into this, we can move to nano precision for the LogAppendTime (with a limit around year 2262).

== Heartbeats to Advance Time

The nodes in the cluster could send small no-op transactions into the log at random intervals to ensure that time moves forward within some reasonable bound. This would be an optional feature, and not necessary to use XT. These messages could indirectly be an indication of which nodes currently are tailing the log, and potentially also a source to track HybridLogicalClock time, though the very existance of the heartbeats would remove the real need for that. It might still be useful for some reason. As the nodes themselves are the source of these messages, there's no need for additional services on servers to generate them. There will be a tension of freshness and filling up and putting load on the log. As our logs don't need infinite retention in Core2 this is less of an issue. The heartbeat messages could be filtered out in the stored version of the log.

== UTC Original Time Zone

All processing has an "original time zone" (as per spec) which is set to UTC. The user can on a per-session basis change this, and of course also insert timestamps with different time zones. CURRENT_DATE, CURRENT_TIME, LOCALTIME and LOCALDATE should be used with care, and with the session time zone set, we may even consider removing these, and only allow literals and explicit casts from CURRENT_TIMESTAMP. Literals are again of course allowed.

The reason for this is that the timezone of the system doesn't represent the timezone(s) that the application operates within. Hence, there's no safe default timezone to assume at the database level. One could of course support system-wide configuration of a different time zone that UTC if this was deemed. In AWS EC2 nodes are set to UTC by default, and this is a bit similar. Without care one can easily get the timezone from the local machine, via the JVM. This choice should always be explicit. Another risk is that different nodes may not have the same timezone, though this is would be very rare in practice.
